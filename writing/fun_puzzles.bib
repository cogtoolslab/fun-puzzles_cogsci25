@book{aalhoPoems,
  title     = "Aum Golly: Poems on Humanity by an Artificial Intelligence",
  author    = "Jukka Aalho and {GPT-3 AI}",
  year      = 2021,
  publisher = "Kertojan {\"a}{\"a}ni"
}

@inproceedings{lourie_unicorn_2021,
	title = {{UNICORN} on {RAINBOW}: {A} {Universal} {Commonsense} {Reasoning} {Model} on a {New} {Multitask} {Benchmark}},
	//url = {https://arxiv.org/abs/2103.13009},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Lourie, Nicholas and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
	year = {2021},
}

@article{callaway2022rational,
  title={Rational use of cognitive resources in human planning},
  author={Callaway, Frederick and van Opheusden, Bas and Gul, Sayan and Das, Priyam and Krueger, Paul M and Griffiths, Thomas L and Lieder, Falk},
  journal={Nature Human Behaviour},
  volume={6},
  number={8},
  pages={1112--1125},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@article{kidd2012goldilocks,
  title={The Goldilocks effect: Human infants allocate attention to visual sequences that are neither too simple nor too complex},
  author={Kidd, Celeste and Piantadosi, Steven T and Aslin, Richard N},
  journal={PloS one},
  volume={7},
  number={5},
  pages={e36399},
  year={2012},
  publisher={Public Library of Science San Francisco, USA}
}

@article{dasgupta2018learning,
  title={Learning to act by integrating mental simulations and physical experiments},
  author={Dasgupta, Ishita and Smith, Kevin A and Schulz, Eric and Tenenbaum, Joshua B and Gershman, Samuel J},
  journal={BioRxiv},
  pages={321497},
  year={2018},
  publisher={Cold Spring Harbor Laboratory}
}

@article{gureckis2012self,
  title={Self-directed learning: A cognitive and computational perspective},
  author={Gureckis, Todd M and Markant, Douglas B},
  journal={Perspectives on Psychological Science},
  volume={7},
  number={5},
  pages={464--481},
  year={2012},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@article{newell1979reasoning,
  title={Reasoning, problem solving and decision processes: The problem space as a fundamental category},
  author={Newell, Allen},
  year={1979},
  publisher={Carnegie Mellon University}
}

@misc{berlyne1960conflict,
  title={Conflict, arousal, and curiosity},
  author={Berlyne, Daniel E},
  year={1960},
  publisher={McGraw-Hill}
}

@article{malone1981toward,
  title={Toward a theory of intrinsically motivating instruction},
  author={Malone, Thomas W},
  journal={Cognitive Science},
  volume={5},
  number={4},
  pages={333--369},
  year={1981},
  publisher={Elsevier}
}

@article{chang_language_2023,
	title = {Language {Model} {Behavior}: {A} {Comprehensive} {Survey}},
	issn = {0891-2017},
	doi = {10.1162/coli_a_00492},
	abstract = {Transformer language models have received widespread public attention, yet their generated text is often surprising even to NLP researchers. In this survey, we discuss over 250 recent studies of English language model behavior before task-specific fine-tuning. Language models possess basic capabilities in syntax, semantics, pragmatics, world knowledge, and reasoning, but these capabilities are sensitive to specific inputs and surface features. Despite dramatic increases in generated text quality as models scale to hundreds of billions of parameters, the models are still prone to unfactual responses, commonsense errors, memorized text, and social biases. Many of these weaknesses can be framed as over-generalizations or under-generalizations of learned patterns in text. We synthesize recent results to highlight what is currently known about large language model capabilities, thus providing a resource for applied work and for research in adjacent fields that use language models.},
	////urldate = {2023-12-26},
	journal = {Computational Linguistics},
	author = {Chang, Tyler A. and Bergen, Benjamin K.},
	//month = nov,
	year = {2023},
	pages = {1--55},
}

@inproceedings{magar_data_2022,
	address = {Dublin, Ireland},
	title = {Data {Contamination}: {From} {Memorization} to {Exploitation}},
	doi = {10.18653/v1/2022.acl-short.18},
	abstract = {Pretrained language models are typically trained on massive web-based datasets, which are often “contaminated” with downstream test sets. It is not clear to what extent models exploit the contaminated data for downstream tasks. We present a principled method to study this question. We pretrain BERT models on joint corpora of Wikipedia and labeled downstream datasets, and fine-tune them on the relevant task. Comparing performance between samples seen and unseen during pretraining enables us to define and quantify levels of memorization and exploitation. Experiments with two models and three downstream tasks show that exploitation exists in some cases, but in others the models memorize the contaminated data, but do not exploit it. We show that these two measures are affected by different factors such as the number of duplications of the contaminated data and the model size. Our results highlight the importance of analyzing massive web-scale datasets to verify that progress in NLP is obtained by better language understanding and not better data exploitation.},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Magar, Inbal and Schwartz, Roy},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	//month = may,
	year = {2022},
	pages = {157--165},
}

@book{guilford_nature_1967,
	title = {The nature of human intelligence},
	author = {Guilford, Joy Paul},
	year = {1967},
	note = {Publisher: McGraw-Hill},
}

@article{fawzi_discovering_2022,
	title = {Discovering faster matrix multiplication algorithms with reinforcement learning},
	volume = {610},
	issn = {1476-4687},
	//url = {https://doi.org/10.1038/s41586-022-05172-4},
	doi = {10.1038/s41586-022-05172-4},
	abstract = {Improving the efficiency of algorithms for fundamental computations can have a widespread impact, as it can affect the overall speed of a large amount of computations. Matrix multiplication is one such primitive task, occurring in many systems—from neural networks to scientific computing routines. The automatic discovery of algorithms using machine learning offers the prospect of reaching beyond human intuition and outperforming the current best human-designed algorithms. However, automating the algorithm discovery procedure is intricate, as the space of possible algorithms is enormous. Here we report a deep reinforcement learning approach based on AlphaZero1 for discovering efficient and provably correct algorithms for the multiplication of arbitrary matrices. Our agent, AlphaTensor, is trained to play a single-player game where the objective is finding tensor decompositions within a finite factor space. AlphaTensor discovered algorithms that outperform the state-of-the-art complexity for many matrix sizes. Particularly relevant is the case of 4 × 4 matrices in a finite field, where AlphaTensor’s algorithm improves on Strassen’s two-level algorithm for the first time, to our knowledge, since its discovery 50 years ago2. We further showcase the flexibility of AlphaTensor through different use-cases: algorithms with state-of-the-art complexity for structured matrix multiplication and improved practical efficiency by optimizing matrix multiplication for runtime on specific hardware. Our results highlight AlphaTensor’s ability to accelerate the process of algorithmic discovery on a range of problems, and to optimize for different criteria.},
	number = {7930},
	journal = {Nature},
	author = {Fawzi, Alhussein and Balog, Matej and Huang, Aja and Hubert, Thomas and Romera-Paredes, Bernardino and Barekatain, Mohammadamin and Novikov, Alexander and R. Ruiz, Francisco J. and Schrittwieser, Julian and Swirszcz, Grzegorz and Silver, David and Hassabis, Demis and Kohli, Pushmeet},
	//month = oct,
	year = {2022},
	pages = {47--53},
}

@article{minsky_why_1982,
	title = {Why {People} {Think} {Computers} {Can}'t},
	volume = {3},
	//url = {https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/376},
	doi = {10.1609/aimag.v3i4.376},
	abstract = {Today, surrounded by so many automatic machines industrial robots, and the R2-D2's of Star wars movies, most people think AI is much more advanced than it is. But still, many \&quot;computer experts\&quot; don\&amp;rsquo;t believe that machines will ever \&quot;really think.\&quot; I think those specialists are too used to explaining that there's nothing inside computers but little electric currents. This leads them to believe that there can\&amp;rsquo;t be room left for anything else- like minds, or selves. And there are many other reasons why so many experts still maintain that machines can never be creative, intuitive, or emotional, and will never really think, believe, or understand anything. This essay explains why they are wrong.},
	number = {4},
	////urldate = {2024-01-31},
	journal = {AI Magazine},
	author = {Minsky, Marvin L.},
	//month = dec,
	year = {1982},
	note = {Section: Articles},
	pages = {3},
}

@incollection{winograd_thinking_1990,
	address = {Cambridge},
	title = {Thinking machines: {Can} there be? {Are} we?},
	isbn = {978-0-521-35944-3},
	//url = {https://www.cambridge.org/core/product/F74198A0B4B517C37414192180B16ED1},
	abstract = {Futurologists have proclaimed the birth of a new species, machina sapiens, that will share (perhaps usurp) our place as the intelligent sovereigns of our earthly domain. These “thinking machines” will take over our burdensome mental chores, just as their mechanical predecessors were intended to eliminate physical drudgery. Eventually they will apply their “ultra-intelligence” to solving all of our problems. Any thoughts of resisting this inevitable evolution is just a form of “speciesism,” born from a romantic and irrational attachment to the peculiarities of the human organism.Critics have argued with equal fervor that “thinking machine” is an oxymoron – a contradiction in terms. Computers, with their foundations of cold logic, can never be creative or insightful or possess real judgment. No matter how competent they appear, they do not have the genuine intentionality that is at the heart of human understanding. The vain pretensions of those who seek to understand mind as computation can be dismissed as yet another demonstration of the arrogance of modern science.Although my own understanding developed through active participation in artificial intelligence research, I have now come to recognize a larger grain of truth in the criticisms than in the enthusiastic predictions. But the story is more complex. The issues need not (perhaps cannot) be debated as fundamental questions concerning the place of humanity in the universe. Indeed, artificial intelligence has not achieved creativity, insight, and judgment. But its shortcomings are far more mundane: we have not yet been able to construct a machine with even a modicum of common sense or one that can converse on everyday topics in ordinary language.},
	booktitle = {The {Foundations} of {Artificial} {Intelligence}: {A} {Sourcebook}},
	publisher = {Cambridge University Press},
	author = {Winograd, Terry},
	editor = {Partridge, Derek and Wilks, Yorick},
	year = {1990},
	doi = {10.1017/CBO9780511663116.017},
	pages = {167--189},
}

@book{dartnall_artificial_1994,
	edition = {1},
	series = {Studies in {Cognitive} {Systems}},
	title = {Artificial {Intelligence} and {Creativity}: {An} {Interdisciplinary} {Approach}},
	//url = {https://doi.org/10.1007/978-94-017-0793-0},
	publisher = {Springer Dordrecht},
	editor = {Dartnall, Terry},
	year = {1994},
}


@misc{taskmaster,
title={Taskmaster [TV Series]},
author = {Alex Horne and Richard Allen-Turner and James Taylor and Jon Thoday and Rob Aslett and {Andy Devonshire (Executive Producers)}},
year = {2015--2019},
publisher = {Dave}}

@misc{openai_gpt-4_2023,
	title = {{GPT}-4 {Technical} {Report}},
	//url = {https://arxiv.org/abs/2303.08774},
	author = {{OpenAI} and {:} and Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and Avila, Red and Babuschkin, Igor and Balaji, Suchir and Balcom, Valerie and Baltescu, Paul and Bao, Haiming and Bavarian, Mo and Belgum, Jeff and Bello, Irwan and Berdine, Jake and Bernadett-Shapiro, Gabriel and Berner, Christopher and Bogdonoff, Lenny and Boiko, Oleg and Boyd, Madelaine and Brakman, Anna-Luisa and Brockman, Greg and Brooks, Tim and Brundage, Miles and Button, Kevin and Cai, Trevor and Campbell, Rosie and Cann, Andrew and Carey, Brittany and Carlson, Chelsea and Carmichael, Rory and Chan, Brooke and Chang, Che and Chantzis, Fotis and Chen, Derek and Chen, Sully and Chen, Ruby and Chen, Jason and Chen, Mark and Chess, Ben and Cho, Chester and Chu, Casey and Chung, Hyung Won and Cummings, Dave and Currier, Jeremiah and Dai, Yunxing and Decareaux, Cory and Degry, Thomas and Deutsch, Noah and Deville, Damien and Dhar, Arka and Dohan, David and Dowling, Steve and Dunning, Sheila and Ecoffet, Adrien and Eleti, Atty and Eloundou, Tyna and Farhi, David and Fedus, Liam and Felix, Niko and Fishman, Simón Posada and Forte, Juston and Fulford, Isabella and Gao, Leo and Georges, Elie and Gibson, Christian and Goel, Vik and Gogineni, Tarun and Goh, Gabriel and Gontijo-Lopes, Rapha and Gordon, Jonathan and Grafstein, Morgan and Gray, Scott and Greene, Ryan and Gross, Joshua and Gu, Shixiang Shane and Guo, Yufei and Hallacy, Chris and Han, Jesse and Harris, Jeff and He, Yuchen and Heaton, Mike and Heidecke, Johannes and Hesse, Chris and Hickey, Alan and Hickey, Wade and Hoeschele, Peter and Houghton, Brandon and Hsu, Kenny and Hu, Shengli and Hu, Xin and Huizinga, Joost and Jain, Shantanu and Jain, Shawn and Jang, Joanne and Jiang, Angela and Jiang, Roger and Jin, Haozhun and Jin, Denny and Jomoto, Shino and Jonn, Billie and Jun, Heewoo and Kaftan, Tomer and Kaiser, Łukasz and Kamali, Ali and Kanitscheider, Ingmar and Keskar, Nitish Shirish and Khan, Tabarak and Kilpatrick, Logan and Kim, Jong Wook and Kim, Christina and Kim, Yongjik and Kirchner, Hendrik and Kiros, Jamie and Knight, Matt and Kokotajlo, Daniel and Kondraciuk, Łukasz and Kondrich, Andrew and Konstantinidis, Aris and Kosic, Kyle and Krueger, Gretchen and Kuo, Vishal and Lampe, Michael and Lan, Ikai and Lee, Teddy and Leike, Jan and Leung, Jade and Levy, Daniel and Li, Chak Ming and Lim, Rachel and Lin, Molly and Lin, Stephanie and Litwin, Mateusz and Lopez, Theresa and Lowe, Ryan and Lue, Patricia and Makanju, Anna and Malfacini, Kim and Manning, Sam and Markov, Todor and Markovski, Yaniv and Martin, Bianca and Mayer, Katie and Mayne, Andrew and McGrew, Bob and McKinney, Scott Mayer and McLeavey, Christine and McMillan, Paul and McNeil, Jake and Medina, David and Mehta, Aalok and Menick, Jacob and Metz, Luke and Mishchenko, Andrey and Mishkin, Pamela and Monaco, Vinnie and Morikawa, Evan and Mossing, Daniel and Mu, Tong and Murati, Mira and Murk, Oleg and Mély, David and Nair, Ashvin and Nakano, Reiichiro and Nayak, Rajeev and Neelakantan, Arvind and Ngo, Richard and Noh, Hyeonwoo and Ouyang, Long and O'Keefe, Cullen and Pachocki, Jakub and Paino, Alex and Palermo, Joe and Pantuliano, Ashley and Parascandolo, Giambattista and Parish, Joel and Parparita, Emy and Passos, Alex and Pavlov, Mikhail and Peng, Andrew and Perelman, Adam and Peres, Filipe de Avila Belbute and Petrov, Michael and Pinto, Henrique Ponde de Oliveira and {Michael} and {Pokorny} and Pokrass, Michelle and Pong, Vitchyr and Powell, Tolly and Power, Alethea and Power, Boris and Proehl, Elizabeth and Puri, Raul and Radford, Alec and Rae, Jack and Ramesh, Aditya and Raymond, Cameron and Real, Francis and Rimbach, Kendra and Ross, Carl and Rotsted, Bob and Roussez, Henri and Ryder, Nick and Saltarelli, Mario and Sanders, Ted and Santurkar, Shibani and Sastry, Girish and Schmidt, Heather and Schnurr, David and Schulman, John and Selsam, Daniel and Sheppard, Kyla and Sherbakov, Toki and Shieh, Jessica and Shoker, Sarah and Shyam, Pranav and Sidor, Szymon and Sigler, Eric and Simens, Maddie and Sitkin, Jordan and Slama, Katarina and Sohl, Ian and Sokolowsky, Benjamin and Song, Yang and Staudacher, Natalie and Such, Felipe Petroski and Summers, Natalie and Sutskever, Ilya and Tang, Jie and Tezak, Nikolas and Thompson, Madeleine and Tillet, Phil and Tootoonchian, Amin and Tseng, Elizabeth and Tuggle, Preston and Turley, Nick and Tworek, Jerry and Uribe, Juan Felipe Cerón and Vallone, Andrea and Vijayvergiya, Arun and Voss, Chelsea and Wainwright, Carroll and Wang, Justin Jay and Wang, Alvin and Wang, Ben and Ward, Jonathan and Wei, Jason and Weinmann, C. J. and Welihinda, Akila and Welinder, Peter and Weng, Jiayi and Weng, Lilian and Wiethoff, Matt and Willner, Dave and Winter, Clemens and Wolrich, Samuel and Wong, Hannah and Workman, Lauren and Wu, Sherwin and Wu, Jeff and Wu, Michael and Xiao, Kai and Xu, Tao and Yoo, Sarah and Yu, Kevin and Yuan, Qiming and Zaremba, Wojciech and Zellers, Rowan and Zhang, Chong and Zhang, Marvin and Zhao, Shengjia and Zheng, Tianhao and Zhuang, Juntang and Zhuk, William and Zoph, Barret},
	year = {2023},
}

@article{brothers_word_2021,
	title = {Word predictability effects are linear, not logarithmic: {Implications} for probabilistic models of sentence comprehension},
	volume = {116},
	issn = {0749-596X},
	//url = {https://www.sciencedirect.com/science/article/pii/S0749596X20300887},
	doi = {10.1016/j.jml.2020.104174},
	abstract = {During language comprehension, we routinely use information from the prior context to help identify the meaning of individual words. While measures of online processing difficulty, such as reading times, are strongly influenced by contextual predictability, there is disagreement about the mechanisms underlying this lexical predictability effect, with different models predicting different linking functions – linear (Reichle, Rayner, \& Pollatsek, 2003) or logarithmic (Levy, 2008). To help resolve this debate, we conducted two highly-powered experiments (self-paced reading, N = 216; cross-modal picture naming, N = 36), and a meta-analysis of prior eye-tracking while reading studies (total N = 218). We observed a robust linear relationship between lexical predictability and word processing times across all three studies. Beyond their methodological implications, these findings also place important constraints on predictive processing models of language comprehension. In particular, these results directly contradict the empirical predictions of surprisal theory, while supporting a proportional pre-activation account of lexical prediction effects in comprehension.},
	journal = {Journal of Memory and Language},
	author = {Brothers, Trevor and Kuperberg, Gina R.},
	//month = feb,
	year = {2021},
	keywords = {Information theory, Prediction, Language comprehension, Psycholinguistics, Reading},
	pages = {104174},
}

@inproceedings{hale_probabilistic_2001,
	title = {A {Probabilistic} {Earley} {Parser} as a {Psycholinguistic} {Model}},
	//url = {https://aclanthology.org/N01-1021},
	booktitle = {Second {Meeting} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}},
	author = {Hale, John},
	year = {2001},
}

@article{michaelov_strong_2023,
	title = {Strong {Prediction}: {Language} {Model} {Surprisal} {Explains} {Multiple} {N400} {Effects}},
	issn = {2641-4368},
	//url = {https://doi.org/10.1162/nol_a_00105},
	doi = {10.1162/nol_a_00105},
	abstract = {Theoretical accounts of the N400 are divided as to whether the amplitude of the N400 response to a stimulus reflects the extent to which the stimulus was predicted, the extent to which the stimulus is semantically similar to its preceding context, or both. We use state-of-the-art machine learning tools to investigate which of these three accounts is best supported by the evidence. GPT-3, a neural language model trained to compute the conditional probability of any word based on the words that precede it, was used to operationalize contextual predictability. In particular, we used an information-theoretic construct known as surprisal (the negative logarithm of the conditional probability). Contextual semantic similarity was operationalized by using two high-quality co-occurrence-derived vector-based meaning representations for words: GloVe and fastText. The cosine between the vector representation of the sentence frame and final word was used to derive contextual cosine similarity estimates. A series of regression models were constructed, where these variables, along with cloze probability and plausibility ratings, were used to predict single trial N400 amplitudes recorded from healthy adults as they read sentences whose final word varied in its predictability, plausibility, and semantic relationship to the likeliest sentence completion. Statistical model comparison indicated GPT-3 surprisal provided the best account of N400 amplitude and suggested that apparently disparate N400 effects of expectancy, plausibility, and contextual semantic similarity can be reduced to variation in the predictability of words. The results are argued to support predictive coding in the human language network.},
	////urldate = {2023-10-31},
	journal = {Neurobiology of Language},
	author = {Michaelov, James A. and Bardolph, Megan D. and Van Petten, Cyma K. and Bergen, Benjamin K. and Coulson, Seana},
	//month = jun,
	year = {2023},
	pages = {1--29},
}

@misc{shain_large-scale_2023,
	title = {Large-{Scale} {Evidence} for {Logarithmic} {Effects} of {Word} {Predictability} on {Reading} {Time}},
	//url = {psyarxiv.com/4hyna},
	publisher = {PsyArXiv},
	author = {Shain, Cory and Meister, Clara and Pimentel, Tiago and Cotterell, Ryan and Levy, Roger P},
	year = {2023},
}

@article{smith_effect_2013,
	title = {The effect of word predictability on reading time is logarithmic},
	volume = {128},
	issn = {0010-0277},
	//url = {http://www.sciencedirect.com/science/article/pii/S0010027713000413},
	doi = {10.1016/j.cognition.2013.02.013},
	abstract = {It is well known that real-time human language processing is highly incremental and context-driven, and that the strength of a comprehender’s expectation for each word encountered is a key determinant of the difficulty of integrating that word into the preceding context. In reading, this differential difficulty is largely manifested in the amount of time taken to read each word. While numerous studies over the past thirty years have shown expectation-based effects on reading times driven by lexical, syntactic, semantic, pragmatic, and other information sources, there has been little progress in establishing the quantitative relationship between expectation (or prediction) and reading times. Here, by combining a state-of-the-art computational language model, two large behavioral data-sets, and non-parametric statistical techniques, we establish for the first time the quantitative form of this relationship, finding that it is logarithmic over six orders of magnitude in estimated predictability. This result is problematic for a number of established models of eye movement control in reading, but lends partial support to an optimal perceptual discrimination account of word recognition. We also present a novel model in which language processing is highly incremental well below the level of the individual word, and show that it predicts both the shape and time-course of this effect. At a more general level, this result provides challenges for both anticipatory processing and semantic integration accounts of lexical predictability effects. And finally, this result provides evidence that comprehenders are highly sensitive to relative differences in predictability – even for differences between highly unpredictable words – and thus helps bring theoretical unity to our understanding of the role of prediction at multiple levels of linguistic structure in real-time language comprehension.},
	number = {3},
	journal = {Cognition},
	author = {Smith, Nathaniel J. and Levy, Roger},
	year = {2013},
	keywords = {Information theory, Psycholinguistics, Expectation, Probabilistic models of cognition, Reading},
	pages = {302 -- 319},
}

@article{johnson_divergent_2023,
	title = {Divergent semantic integration ({DSI}): {Extracting} creativity from narratives with distributional semantic modeling},
	volume = {55},
	issn = {1554-3528},
	//url = {https://doi.org/10.3758/s13428-022-01986-2},
	doi = {10.3758/s13428-022-01986-2},
	abstract = {We developed a novel conceptualization of one component of creativity in narratives by integrating creativity theory and distributional semantics theory. We termed the new construct divergent semantic integration (DSI), defined as the extent to which a narrative connects divergent ideas. Across nine studies, 27 different narrative prompts, and over 3500 short narratives, we compared six models of DSI that varied in their computational architecture. The best-performing model employed Bidirectional Encoder Representations from Transformers (BERT), which generates context-dependent numerical representations of words (i.e., embeddings). BERT DSI scores demonstrated impressive predictive power, explaining up to 72\% of the variance in human creativity ratings, even approaching human inter-rater reliability for some tasks. BERT DSI scores showed equivalently high predictive power for expert and nonexpert human ratings of creativity in narratives. Critically, DSI scores generalized across ethnicity and English language proficiency, including individuals identifying as Hispanic and L2 English speakers. The integration of creativity and distributional semantics theory has substantial potential to generate novel hypotheses about creativity and novel operationalizations of its underlying processes and components. To facilitate new discoveries across diverse disciplines, we provide a tutorial with code (osf.io/ath2s) on how to compute DSI and a web app (osf.io/ath2s) to freely retrieve DSI scores.},
	number = {7},
	journal = {Behavior Research Methods},
	author = {Johnson, Dan R. and Kaufman, James C. and Baker, Brendan S. and Patterson, John D. and Barbot, Baptiste and Green, Adam E. and van Hell, Janet and Kennedy, Evan and Sullivan, Grace F. and Taylor, Christa L. and Ward, Thomas and Beaty, Roger E.},
	//month = oct,
	year = {2023},
	pages = {3726--3759},
}

@inproceedings{pennington_glove_2014,
  address   = {Doha, Qatar},
  title     = {{GloVe}: {Global} {Vectors} for {Word} {Representation}},
  url       = {https://aclanthology.org/D14-1162},
  doi       = {10.3115/v1/D14-1162},
  booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
  publisher = {Association for Computational Linguistics},
  author    = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  month     = oct,
  year      = {2014},
  pages     = {1532--1543}
}

@misc{radford_language_2019,
  title  = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
  url    = {https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf},
  author = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
 journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
   year   = {2019}
}

@inproceedings{lee_lftk_2023,
  address   = {Toronto, Canada},
  title     = {{LFTK}: {Handcrafted} {Features} in {Computational} {Linguistics}},
  url       = {https://aclanthology.org/2023.bea-1.1},
  abstract  = {Past research has identified a rich set of handcrafted linguistic features that can potentially assist various tasks. However, their extensive number makes it difficult to effectively select and utilize existing handcrafted features. Coupled with the problem of inconsistent implementation across research works, there has been no categorization scheme or generally-accepted feature names. This creates unwanted confusion. Also, no actively-maintained open-source library extracts a wide variety of handcrafted features. The current handcrafted feature extraction practices have several inefficiencies, and a researcher often has to build such an extraction system from the ground up. We collect and categorize more than 220 popular handcrafted features grounded on past literature. Then, we conduct a correlation analysis study on several task-specific datasets and report the potential use cases of each feature. Lastly, we devise a multilingual handcrafted linguistic feature extraction system in a systematically expandable manner. We open-source our system to give the community a rich set of pre-implemented handcrafted features.},
  booktitle = {Proceedings of the 18th {Workshop} on {Innovative} {Use} of {NLP} for {Building} {Educational} {Applications} ({BEA} 2023)},
  publisher = {Association for Computational Linguistics},
  author    = {Lee, Bruce W. and Lee, Jason},
  month     = jul,
  year      = {2023},
  pages     = {1--19}
}

@misc{honnibal_spacy_2020,
  title     = {{spaCy}: {Industrial}-strength {Natural} {Language} {Processing} in {Python}},
  doi       = {10.5281/zenodo.1212303},
  publisher = {Zenodo},
  author    = {Honnibal, Matthew and Montani, Ines and Van Landeghem, Sofie and Boyd, Adriane},
  year      = {2020}
}

@article{genesereth2005general,
  title   = {General game playing: Overview of the AAAI competition},
  author  = {Genesereth, Michael and Love, Nathaniel and Pell, Barney},
  journal = {AI magazine},
  volume  = {26},
  number  = {2},
  pages   = {62--62},
  year    = {2005}
}

@misc{veselovsky2023artificial,
  title         = {Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks},
  author        = {Veniamin Veselovsky and Manoel Horta Ribeiro and Robert West},
  year          = {2023},
  eprint        = {2306.07899},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@article{maas2005sufficient,
  title     = {Sufficient sample sizes for multilevel modeling},
  author    = {Maas, Cora JM and Hox, Joop J},
  journal   = {Methodology},
  volume    = {1},
  number    = {3},
  pages     = {86--92},
  year      = {2005},
  publisher = {Hogrefe \& Huber Publishers}
}

@article{acerbiLargeLanguageModels2023,
  title        = {Large Language Models Show Human-like Content Biases in Transmission Chain Experiments},
  author       = {Acerbi, Alberto and Stubbersfield, Joseph M.},
  year         = {2023},
  journaltitle = {Proc. Natl. Acad. Sci. U.S.A.},
  volume       = {120},
  number       = {44},
  pages        = {e2313790120},
  issn         = {0027-8424, 1091-6490},
  doi          = {10/gtftvm},
  url          = {https://pnas.org/doi/10.1073/pnas.2313790120},
  //urldate      = {2024-01-25},
  abstract     = {As the use of large language models (LLMs) grows, it is important to examine whether they exhibit biases in their output. Research in cultural evolution, using transmission chain experiments, demonstrates that humans have biases to attend to, remember, and transmit some types of content over others. Here, in five preregistered experiments using material from previous studies with human participants, we use the same, transmission chain-like methodology, and find that the LLM ChatGPT-3 shows biases analogous to humans for content that is gender-stereotype-consistent, social, negative, threat-related, and biologically counterintuitive, over other content. The presence of these biases in LLM output suggests that such content is widespread in its training data and could have consequential downstream effects, by magnifying preexisting human tendencies for cognitively appealing and not necessarily informative, or valuable, content.},
  langid       = {english},
  file         = {/Users/junyichu/Zotero/storage/HNGK7U4T/acerbi_stubbersfield.pdf}
}

@book{boden2004creative,
  title={The creative mind: Myths and mechanisms},
  author={Boden, Margaret A},
  year={2004},
  publisher={Psychology Press}
}

@misc{franceschelliCreativityLargeLanguage2023,
  title       = {On the {{Creativity}} of {{Large Language Models}}},
  author      = {Franceschelli, Giorgio and Musolesi, Mirco},
  year        = {2023},
  eprint      = {2304.00008},
  eprinttype  = {arxiv},
  eprintclass = {cs},
  url         = {http://arxiv.org/abs/2304.00008},
  //urldate     = {2024-01-25},
  abstract    = {Large Language Models (LLMs) are revolutionizing several areas of Artificial Intelligence. One of the most remarkable applications is creative writing, e.g., poetry or storytelling: the generated outputs are often of astonishing quality. However, a natural question arises: can LLMs be really considered creative? In this article we firstly analyze the development of LLMs under the lens of creativity theories, investigating the key open questions and challenges. In particular, we focus our discussion around the dimensions of value, novelty and surprise as proposed by Margaret Boden in her work. Then, we consider different classic perspectives, namely product, process, press and person. We discuss a set of “easy” and “hard” problems in machine creativity, presenting them in relation to LLMs. Finally, we examine the societal impact of these technologies with a particular focus on the creative industries, analyzing the opportunities offered by them, the challenges arising by them and the potential associated risks, from both legal and ethical points of view.},
  langid      = {english},
  pubstate    = {preprint},
  keywords    = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society},
  file        = {/Users/junyichu/Zotero/storage/XTY3J952/Franceschelli and Musolesi - 2023 - On the Creativity of Large Language Models.pdf}
}

@article{SILVIA201124,
title = {Subjective scoring of divergent thinking: Examining the reliability of unusual uses, instances, and consequences tasks},
journal = {Thinking Skills and Creativity},
volume = {6},
number = {1},
pages = {24-30},
year = {2011},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2010.06.001},
//url = {https://www.sciencedirect.com/science/article/pii/S1871187110000295},
author = {Paul J. Silvia},
keywords = {Creativity, Divergent thinking, Measurement, Reliability, Latent variable models},
abstract = {The present research examined the reliability of three types of divergent thinking tasks (unusual uses, instances, consequences/implications) and two types of subjective scoring (an average across all responses vs. the responses people chose as their top-two responses) within a latent variable framework, using the maximal-reliability H statistic. Overall, the unusual uses tasks performed the best for both scoring types, the instances tasks had less reliable scores, and the consequences tasks had poor reliability and convergence problems. The discussion considers implications for test users, differences between average scoring and top-two scoring, and the problem of whether divergent thinking tasks are interchangeable.}
}

@article{gilhooly2007divergent,
  title={Divergent thinking: Strategies and executive involvement in generating novel uses for familiar objects},
  author={Gilhooly, Kenneth J and Fioratou, Evridiki and Anthony, Susan H and Wynn, Victor},
  journal={British Journal of Psychology},
  volume={98},
  number={4},
  pages={611--625},
  year={2007},
  publisher={Wiley Online Library}
}
@article{gilhoolyAIVsHumans2024,
  title        = {{{AI}} vs Humans in the {{AUT}}: {{Simulations}} to {{LLMs}}},
  shorttitle   = {{{AI}} vs Humans in the {{AUT}}},
  author       = {Gilhooly, Kenneth J},
  year         = {2024},
  journal = {Journal of Creativity},
volume       = {34},
  number       = {1},
  pages        = {100071},
  issn         = {27133745},
  doi          = {10/gtfttp},
  abstract     = {This paper reviews studies of proposed creative machines applied to a prototypical creative task, i.e., the Alternative Uses Task (AUT). Although one system (OROC) did simulate some aspects of human strategies for the AUT, most recent attempts have not been simulation-oriented, but rather have used Large Language Model (LLM) systems such as GPT-3 which embody extremely large connectionist networks trained on huge volumes of textual data. Studies reviewed here indicate that LLM based systems are performing on the AUT at near or somewhat above human levels in terms of scores on originality and usefulness. Moreover, similar patterns are found in the data of humans and LLM models in the AUT, such as output order effects and a negative association between originality and value or utility. However, it is concluded that GPT-3 and similar systems, despite generating novel and useful responses, do not display creativity as they lack agency and are purely algorithmic. LLM studies so far in this area have largely been exploratory and future studies should guard against possible training data contamination.}
}

@inproceedings{goesPushingGPTCreativity2023,
  title     = {Pushing {{GPT}}’s {{Creativity}} to {{Its Limits}}: {{Alternative Uses}} and {{Torrance Tests}}},
  booktitle = {Proceedings of the 14th {{International Conference}} for {{Computational Creativity}}},
  author    = {Goes, Fabricio and Volpe, Marco and Sawicki, Piotr and Grzes, Marek and Watson, Jacob},
  year      = {2023},
  abstract  = {In this paper, we investigate the potential of Large Language Models (LLMs), specifically GPT-4, to improve their creative responses in well-known creativity tests, such as Guilford
               ’s Alternative Uses Test (AUT) and an adapted version of the Torrance Test of Creative Thinking (TTCT) visual completion tests. We exploit GPT-4’s self-improving ability by using a sequence of forceful interactive prompts in a multistep conversation, aiming to accelerate the convergence process towards more creative responses. Our contributions include an automated approach to enhance GPT’s responses in the AUT and TTCT visual completion test and a series of prompts to generate and evaluate GPT’s responses in these tests. Our results show that the creativity of GPT’s responses can be improved through the use of forceful prompts. This paper opens up possibilities for future research on different sets of prompts to further improve the creativity convergence of LLM-generated responses and the application of similar interactive processes to tasks involving other cognitive skills.},
  langid    = {english},
  keywords  = {⛔ No DOI found},
  file      = {/Users/junyichu/Zotero/storage/M4QI9FH5/Goes et al. - Pushing GPT’s Creativity to Its Limits Alternativ.pdf}
}

@article{jordanousModellingCreativityIdentifying2016,
  title        = {Modelling {{Creativity}}: {{Identifying Key Components}} through a {{Corpus-Based Approach}}},
  shorttitle   = {Modelling {{Creativity}}},
  author       = {Jordanous, Anna and Keller, Bill},
  editor       = {Csermely, Peter},
  year         = {2016},
  journaltitle = {PLoS ONE},
  volume       = {11},
  number       = {10},
  pages        = {e0162959},
  issn         = {1932-6203},
  doi          = {10/f9rpqn},
  url          = {https://dx.plos.org/10.1371/journal.pone.0162959},
  //urldate      = {2024-01-25},
  langid       = {english},
  file         = {/Users/junyichu/Zotero/storage/F9RIWWLC/Jordanous and Keller - 2016 - Modelling Creativity Identifying Key Components t.pdf}
}

@article{keExploringFrontiersLLMs2024,
  title      = {Exploring the {{Frontiers}} of {{LLMs}} in {{Psychological Applications}}: {{A Comprehensive Review}}},
  shorttitle = {Exploring the {{Frontiers}} of {{LLMs}} in {{Psychological Applications}}},
  author     = {Ke, Luoma and Tong, Song and Cheng, Peng and Peng, Kaiping},
  year       = {2024},
  publisher  = {{arXiv}},
  doi        = {10/gtftvp},
  url        = {https://arxiv.org/abs/2401.01519},
  //urldate    = {2024-01-25},
  abstract   = {This paper explores the frontiers of large language models (LLMs) in psychology applications. Psychology has undergone several theoretical changes, and the current use of Artificial Intelligence (AI) and Machine Learning, particularly LLMs, promises to open up new research directions. We provide a detailed exploration of how LLMs like ChatGPT are transforming psychological research. It discusses the impact of LLMs across various branches of psychology, including cognitive and behavioral, clinical and counseling, educational and developmental, and social and cultural psychology, highlighting their potential to simulate aspects of human cognition and behavior. The paper delves into the capabilities of these models to emulate human-like text generation, offering innovative tools for literature review, hypothesis generation, experimental design, experimental subjects, data analysis, academic writing, and peer review in psychology. While LLMs are essential in advancing research methodologies in psychology, the paper also cautions about their technical and ethical challenges. There are issues like data privacy, the ethical implications of using LLMs in psychological research, and the need for a deeper understanding of these models' limitations. Researchers should responsibly use LLMs in psychological studies, adhering to ethical standards and considering the potential consequences of deploying these technologies in sensitive areas. Overall, the article provides a comprehensive overview of the current state of LLMs in psychology, exploring potential benefits and challenges. It serves as a call to action for researchers to leverage LLMs' advantages responsibly while addressing associated risks.},
  version    = {2},
  keywords   = {Artificial Intelligence (cs.AI),FOS: Computer and information sciences,Machine Learning (cs.LG)}
}

@article{koivistoBestHumansStill2023,
  title={Best humans still outperform artificial intelligence in a creative divergent thinking task},
  author={Koivisto, Mika and Grassini, Simone},
  journal={Scientific reports},
  volume={13},
  number={1},
  pages={13601},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{Patterson2023,
  title = {AuDrA: An automated drawing assessment platform for evaluating creativity},
  ISSN = {1554-3528},
  DOI = {10.3758/s13428-023-02258-3},
  journal = {Behavior Research Methods},
  publisher = {Springer Science and Business Media LLC},
  author = {Patterson,  John D. and Barbot,  Baptiste and Lloyd-Cox,  James and Beaty,  Roger E.},
  year = {2023},
}

@article{Beaty2019,
  title = {Network neuroscience of creative cognition: mapping cognitive mechanisms and individual differences in the creative brain},
  volume = {27},
  ISSN = {2352-1546},
  //url = {http://dx.doi.org/10.1016/j.cobeha.2018.08.013},
  DOI = {10.1016/j.cobeha.2018.08.013},
  journal = {Current Opinion in Behavioral Sciences},
  publisher = {Elsevier BV},
  author = {Beaty,  Roger E and Seli,  Paul and Schacter,  Daniel L},
  year = {2019},
  pages = {22–30}
}

@article{orwigLanguageCreativityEvidence2024,
  title        = {The {{Language}} of {{Creativity}}: {{Evidence}} from {{Humans}} and {{Large Language Models}}},
  shorttitle   = {The {{Language}} of {{Creativity}}},
  author       = {Orwig, William and Edenbaum, Emma R. and Greene, Joshua D. and Schacter, Daniel L.},
  year         = {2024},
  journal = {Journal of Creative Behavior},
  pages        = {jocb.636},
  issn         = {0022-0175, 2162-6057},
  doi          = {10/gtftvr},
}

@article{Guzik2023,
  title = {The originality of machines: AI takes the Torrance Test},
  volume = {33},
  ISSN = {2713-3745},
  //url = {http://dx.doi.org/10.1016/j.yjoc.2023.100065},
  DOI = {10.1016/j.yjoc.2023.100065},
  number = {3},
  journal = {Journal of Creativity},
  publisher = {Elsevier BV},
  author = {Guzik,  Erik E. and Byrge,  Christian and Gilde,  Christian},
  year = {2023},
  //month = dec,
  pages = {100065}
}

@article{haase2023artificial,
  title={Artificial muses: Generative artificial intelligence chatbots have risen to human-level creativity},
  author={Haase, Jennifer and Hanel, Paul HP},
  journal={arXiv preprint arXiv:2303.12003},
  year={2023}
}

@article{stevenson2022putting,
  title={Putting GPT-3's Creativity to the (Alternative Uses) Test},
  author={Stevenson, Claire and Smal, Iris and Baas, Matthijs and Grasman, Raoul and van der Maas, Han},
  journal={arXiv preprint arXiv:2206.08932},
  year={2022}
}

@misc{zhao2024,
  title       = {Assessing and {{Understanding Creativity}} in {{Large Language Models}}},
  author      = {Zhao, Yunpu and Zhang, Rui and Li, Wenyi and Huang, Di and Guo, Jiaming and Peng, Shaohui and Hao, Yifan and Wen, Yuanbo and Hu, Xing and Du, Zidong and Guo, Qi and Li, Ling and Chen, Yunji},
  year        = {2024},
  eprint      = {2401.12491},
  eprinttype  = {arxiv},
  eprintclass = {cs},
  url         = {http://arxiv.org/abs/2401.12491},
  abstract    = {In the field of natural language processing, the rapid development of large language model (LLM) has attracted more and more attention. LLMs have shown a high level of creativity in various tasks, but the methods for assessing such creativity are inadequate. The assessment of LLM creativity needs to consider differences from humans, requiring multi-dimensional measurement while balancing accuracy and efficiency. This paper aims to establish an efficient framework for assessing the level of creativity in LLMs. By adapting the modified Torrance Tests of Creative Thinking, the research evaluates the creative performance of various LLMs across 7 tasks, emphasizing 4 criteria including Fluency, Flexibility, Originality, and Elaboration. In this context, we develop a comprehensive dataset of 700 questions for testing and an LLM-based evaluation method. In addition, this study presents a novel analysis of LLMs' responses to diverse prompts and role-play situations. We found that the creativity of LLMs primarily falls short in originality, while excelling in elaboration. Besides, the use of prompts and the role-play settings of the model significantly influence creativity. Additionally, the experimental results also indicate that collaboration among multiple LLMs can enhance originality. Notably, our findings reveal a consensus between human evaluations and LLMs regarding the personality traits that influence creativity. The findings underscore the significant impact of LLM design on creativity and bridges artificial intelligence and human creativity, offering insights into LLMs' creativity and potential applications.},
  pubstate    = {preprint},
  keywords    = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file        = {/Users/junyichu/Zotero/storage/BQDJDH5I/Zhao et al. - 2024 - Assessing and Understanding Creativity in Large La.pdf;/Users/junyichu/Zotero/storage/8ADLBI6H/2401.html}
}

@article{zhuGenerativeTransformersDesign2022,
  title        = {Generative {{Transformers}} for {{Design Concept Generation}}},
  author       = {Zhu, Qihao and Luo, Jianxi},
  date         = {2022-11-11},
  journal = {Journal of Computing and Information Science in Engineering},
  pages        = {1--61},
  issn         = {1530-9827, 1944-7078},
  doi          = {10/gtftvn},
  url          = {https://asmedigitalcollection.asme.org/computingengineering/article/doi/10.1115/1.4056220/1150228/Generative-Transformers-for-Design-Concept},
  //urldate      = {2024-01-25},
  abstract     = {Abstract             Generating novel and useful concepts is essential during the early design stage to explore a large variety of design opportunities, which usually requires advanced design thinking ability and a wide range of knowledge from designers. Growing works on computer-aided tools have explored the retrieval of knowledge and heuristics from design data. However, they only provide stimuli to inspire designers from limited aspects. This study explores the recent advance of the natural language generation (NLG) technique in the artificial intelligence (AI) field to automate the early-stage design concept generation. Specifically, a novel approach utilizing the generative pre-trained transformer (GPT) is proposed to leverage the knowledge and reasoning from textual data and transform them into new concepts in understandable language. Three concept generation tasks are defined to leverage different knowledge and reasoning: domain knowledge synthesis, problem-driven synthesis, and analogy-driven synthesis. The experiments with both human and data-driven evaluation show good performance in generating novel and useful concepts.},
  langid       = {english},
  file         = {/Users/junyichu/Zotero/storage/TU34KQ2F/Zhu and Luo - 2022 - Generative Transformers for Design Concept Generat.pdf}
}

@article{ludecke_performance_2021,
  title   = {{performance}: An {R} Package for Assessment, Comparison and Testing of Statistical Models},
  author  = {Daniel Lüdecke and Mattan S. Ben-Shachar and Indrajeet Patil and Philip Waggoner and Dominique Makowski},
  year    = {2021},
  journal = {Journal of Open Source Software},
  volume  = {6},
  number  = {60},
  pages   = {3139},
  doi     = {10.21105/joss.03139}
}

% CREATIVITY JUDGMENT

@article{Acar2017,
  title = {Ingredients of Creativity: Originality and More},
  volume = {29},
  ISSN = {1532-6934},
  //url = {http://dx.doi.org/10.1080/10400419.2017.1302776},
  DOI = {10.1080/10400419.2017.1302776},
  number = {2},
  journal = {Creativity Research Journal},
  publisher = {Informa UK Limited},
  author = {Acar,  Selcuk and Burnett,  Cyndi and Cabra,  John F.},
  year = {2017},
  //month = apr,
  pages = {133–144}
}

@article{Runco2012,
  title = {The Standard Definition of Creativity},
  volume = {24},
  ISSN = {1532-6934},
  //url = {http://dx.doi.org/10.1080/10400419.2012.650092},
  DOI = {10.1080/10400419.2012.650092},
  number = {1},
  journal = {Creativity Research Journal},
  publisher = {Informa UK Limited},
  author = {Runco,  Mark A. and Jaeger,  Garrett J.},
  year = {2012},
  //month = jan,
  pages = {92–96}
}

@article{Simonton2000,
  title = {Creativity: Cognitive,  personal,  developmental,  and social aspects.},
  volume = {55},
  ISSN = {0003-066X},
  //url = {http://dx.doi.org/10.1037/0003-066X.55.1.151},
  DOI = {10.1037/0003-066x.55.1.151},
  number = {1},
  journal = {American Psychologist},
  publisher = {American Psychological Association (APA)},
  author = {Simonton,  Dean Keith},
  year = {2000},
  pages = {151–158}
}

@article{ORGANISCIAK2023101356,
title = {Beyond semantic distance: Automated scoring of divergent thinking greatly improves with large language models},
journal = {Thinking Skills and Creativity},
volume = {49},
pages = {101356},
year = {2023},
issn = {1871-1871},
doi = {10.1016/j.tsc.2023.101356},
//url = {https://www.sciencedirect.com/science/article/pii/S1871187123001256},
author = {Peter Organisciak and Selcuk Acar and Denis Dumas and Kelly Berthiaume},
keywords = {Divergent thinking, Alternate uses test, Large-language models, Automated scoring},
}

@article{Dumas2021,
  title = {Measuring divergent thinking originality with human raters and text-mining models: A psychometric comparison of methods.},
  volume = {15},
  ISSN = {1931-3896},
  //url = {http://dx.doi.org/10.1037/aca0000319},
  DOI = {10.1037/aca0000319},
  number = {4},
  journal = {Psychology of Aesthetics,  Creativity,  and the Arts},
  publisher = {American Psychological Association (APA)},
  author = {Dumas,  Denis and Organisciak,  Peter and Doherty,  Michael},
  year = {2021},
  //month = nov,
  pages = {645–663}
}

@article{Kaufman2008,
  title = {A Comparison of Expert and Nonexpert Raters Using the Consensual Assessment Technique},
  volume = {20},
  ISSN = {1532-6934},
  //url = {http://dx.doi.org/10.1080/10400410802059929},
  DOI = {10.1080/10400410802059929},
  number = {2},
  journal = {Creativity Research Journal},
  publisher = {Informa UK Limited},
  author = {Kaufman,  James C. and Baer,  John and Cole,  Jason C. and Sexton,  Janel D.},
  year = {2008},
  //month = may,
  pages = {171–178}
}

% COMPUTATIONAL MEASURES OF CREATIVITY

@article{Beaty2020,
  title = {Automating creativity assessment with SemDis: An open platform for computing semantic distance},
  volume = {53},
  ISSN = {1554-3528},
  //url = {http://dx.doi.org/10.3758/s13428-020-01453-w},
  DOI = {10.3758/s13428-020-01453-w},
  number = {2},
  journal = {Behavior Research Methods},
  publisher = {Springer Science and Business Media LLC},
  author = {Beaty,  Roger E. and Johnson,  Dan R.},
  year = {2020},
  //month = aug,
  pages = {757–780}
}

@article{Zedelius2018,
  title = {Beyond subjective judgments: Predicting evaluations of creative writing from computational linguistic features},
  volume = {51},
  ISSN = {1554-3528},
  //url = {http://dx.doi.org/10.3758/s13428-018-1137-1},
  DOI = {10.3758/s13428-018-1137-1},
  number = {2},
  journal = {Behavior Research Methods},
  publisher = {Springer Science and Business Media LLC},
  author = {Zedelius,  Claire M. and Mills,  Caitlin and Schooler,  Jonathan W.},
  year = {2018},
  //month = sep,
  pages = {879–894}
}
@article{zhang2024people,
  title={People use fast, goal-directed simulation to reason about novel games},
  author={Zhang, Cedegao E and Collins, Katherine M and Wong, Lionel and Weller, Adrian and Tenenbaum, Joshua B},
  journal={arXiv preprint arXiv:2407.14095},
  year={2024}
}


@article{Yang2022,
  title = {Predication of Writing Originality Based on Computational Linguistics},
  volume = {10},
  ISSN = {2079-3200},
  //url = {http://dx.doi.org/10.3390/jintelligence10040124},
  DOI = {10.3390/jintelligence10040124},
  number = {4},
  journal = {Journal of Intelligence},
  publisher = {MDPI AG},
  author = {Yang,  Liping and Xin,  Tao and Zhang,  Sheng and Yu,  Yunye},
  year = {2022},
  //month = dec,
  pages = {124}
}

@article{Ahmed2021,
  title = {The Language of Creativity: Validating Linguistic Analysis to Assess Creative Scientists and Artists},
  volume = {12},
  ISSN = {1664-1078},
  //url = {http://dx.doi.org/10.3389/fpsyg.2021.724083},
  DOI = {10.3389/fpsyg.2021.724083},
  journal = {Frontiers in Psychology},
  publisher = {Frontiers Media SA},
  author = {Ahmed,  Sana Tariq and Feist,  Gregory J.},
  year = {2021},
  //month = nov 
}

@article{Kbis2021,
  title = {Artificial intelligence versus Maya Angelou: Experimental evidence that people cannot differentiate AI-generated from human-written poetry},
  volume = {114},
  ISSN = {0747-5632},
  //url = {http://dx.doi.org/10.1016/j.chb.2020.106553},
  DOI = {10.1016/j.chb.2020.106553},
  journal = {Computers in Human Behavior},
  publisher = {Elsevier BV},
  author = {K\"{o}bis,  Nils and Mossink,  Luca D.},
  year = {2021},
  //month = jan,
  pages = {106553}
}

@inbook{Plucker2010,
  title = {Assessment of Creativity},
  ISBN = {9780521730259},
  //url = {http://dx.doi.org/10.1017/CBO9780511763205.005},
  DOI = {10.1017/cbo9780511763205.005},
  booktitle = {The Cambridge Handbook of Creativity},
  publisher = {Cambridge University Press},
  author = {Plucker,  Jonathan A. and Makel,  Matthew C.},
  year = {2010},
  //month = aug,
  pages = {48–73}
}
%% AUT

@article{hass2017semantic,
  title={Semantic search during divergent thinking},
  author={Hass, Richard W},
  journal={Cognition},
  volume={166},
  pages={344--357},
  year={2017},
  publisher={Elsevier}
}

@article{runco1992children,
  title={Children's divergent thinking and creative ideation},
  author={Runco, Mark A},
  journal={Developmental Review},
  volume={12},
  number={3},
  pages={233--264},
  year={1992},
  publisher={Elsevier}
}

@article{bijvoet2014individual,
  title={Individual differences and age-related changes in divergent thinking in toddlers and preschoolers.},
  author={Bijvoet-van Den Berg, Simone and Hoicka, Elena},
  journal={Developmental psychology},
  volume={50},
  number={6},
  pages={1629},
  year={2014},
  publisher={American Psychological Association}
}

@book{sternberg1999handbook,
  title={Handbook of creativity},
  author={Sternberg, Robert J},
  year={1999},
  publisher={Cambridge University Press}
}

@article{Hass2018,
  title = {On the Dependability and Feasibility of Layperson Ratings of Divergent Thinking},
  volume = {9},
  ISSN = {1664-1078},
  //url = {http://dx.doi.org/10.3389/fpsyg.2018.01343},
  DOI = {10.3389/fpsyg.2018.01343},
  journal = {Frontiers in Psychology},
  publisher = {Frontiers Media SA},
  author = {Hass,  Richard W. and Rivera,  Marisa and Silvia,  Paul J.},
  year = {2018},
  //month = aug 
}

@article{kaufman2013furious,
  title={Furious activity vs. understanding: How much expertise is needed to evaluate creative work?},
  author={Kaufman, James C and Baer, John and Cropley, David H and Reiter-Palmon, Roni and Sinnett, Sarah},
  journal={Psychology of Aesthetics, Creativity, and the Arts},
  volume={7},
  number={4},
  pages={332},
  year={2013},
  publisher={Educational Publishing Foundation}
}

%% PROBLEM FINDING
@incollection{getzels1975problemfinding,
  title={From problem solving to problem finding},
  author={Getzels, Jacob W and Csikszentmihalyi, Mihaly},
  booktitle={Perspectives in creativity},
  pages={90--116},
  year={1975},
  publisher={Routledge}
}
@article{hu2010creative,
  title={Creative scientific problem finding and its developmental trend},
  author={Hu, Weiping and Shi, Quan Zhen and Han, Qin and Wang, Xingqi and Adey, Philip},
  journal={Creativity Research Journal},
  volume={22},
  number={1},
  pages={46--52},
  year={2010},
  publisher={Taylor \& Francis}
}

@article{runco1994giftedness,
  title={Problem finding, creativity, and giftedness},
  author={Runco, Mark A and Nemiro, Jill},
  journal={Roeper Review},
  volume={16},
  number={4},
  pages={235--241},
  year={1994},
  publisher={Taylor \& Francis}
}

@book{runco1994problemfinding,
  title={Problem finding, problem solving, and creativity},
  author={Runco, Mark A},
  year={1994},
  publisher={Greenwood Publishing Group}
}

@article{runco1988problem,
  title={Problem discovery, divergent thinking, and the creative process},
  author={Runco, Mark A and Okuda, Shawn M},
  journal={Journal of youth and adolescence},
  volume={17},
  number={3},
  pages={211--220},
  year={1988},
  publisher={Springer}
}

@article{okuda1991creativity,
  title={Creativity and the finding and solving of real-world problems},
  author={Okuda, Shawn M and Runco, Mark A and Berger, Dale E},
  journal={Journal of Psychoeducational assessment},
  volume={9},
  number={1},
  pages={45--53},
  year={1991},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}

@article{Abdulla2018,
  title = {The Creative Problem Finding Hierarchy: A Suggested Model for Understanding Problem Finding},
  volume = {5},
  ISSN = {2354-0036},
  //url = {http://dx.doi.org/10.1515/ctra-2018-0019},
  DOI = {10.1515/ctra-2018-0019},
  number = {2},
  journal = {Creativity. Theories – Research - Applications},
  publisher = {Walter de Gruyter GmbH},
  author = {Abdulla,  Ahmed M. and Cramond,  Bonnie},
  year = {2018},
  //month = dec,
  pages = {197–229}
}

@article{Abdulla2020,
  title = {Problem finding and creativity: A meta-analytic review.},
  volume = {14},
  ISSN = {1931-3896},
  //url = {http://dx.doi.org/10.1037/aca0000194},
  DOI = {10.1037/aca0000194},
  number = {1},
  journal = {Psychology of Aesthetics,  Creativity,  and the Arts},
  publisher = {American Psychological Association (APA)},
  author = {Abdulla,  Ahmed M. and Paek,  Sue Hyeon and Cramond,  Bonnie and Runco,  Mark A.},
  year = {2020},
  //month = feb,
  pages = {3–14}
}

@article{abdulla2021problem,
  title={Problem finding, divergent thinking, and evaluative thinking among gifted and nongifted students},
  author={Abdulla Alabbasi, Ahmed M and Hafsyan, Amnah SM and Runco, Mark A and AlSaleh, Aseel},
  journal={Journal for the Education of the Gifted},
  volume={44},
  number={4},
  pages={398--413},
  year={2021},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{wakefield1985towards,
  title={Towards creativity: Problem finding in a divergent-thinking exercise.},
  author={Wakefield, John F},
  year={1985},
  publisher={ERIC},
journal = {Child Study Journal},
vol = 15,
issue=4
}

@article{russ2010developmental,
  title={Developmental approaches to creativity},
  author={Russ, Sandra W and Fiorelli, Julie A},
  journal={The Cambridge handbook of creativity},
  volume={12},
  pages={233--249},
  year={2010}
}

@article{Csikszentmihalyi1971,
  title = {Discovery-oriented behavior and the originality of creative products: A study with artists.},
  volume = {19},
  ISSN = {0022-3514},
  //url = {http://dx.doi.org/10.1037/h0031106},
  DOI = {10.1037/h0031106},
  number = {1},
  journal = {Journal of Personality and Social Psychology},
  publisher = {American Psychological Association (APA)},
  author = {Csikszentmihalyi,  M. and Getzels,  J. W.},
  year = {1971},
  pages = {47–52}
}

@book{Dewey1910,
  title = {How we think.},
  //url = {http://dx.doi.org/10.1037/10903-000},
  DOI = {10.1037/10903-000},
  publisher = {D C Heath},
  author = {Dewey,  John},
  year = {1910}
}

@article{schmidhuber2010formal,
  title={Formal theory of creativity, fun, and intrinsic motivation (1990--2010)},
  author={Schmidhuber, J{\"u}rgen},
  journal={IEEE transactions on autonomous mental development},
  volume={2},
  number={3},
  pages={230--247},
  year={2010},
  publisher={Ieee}
}

@article{Trinh2024,
  title = {Solving olympiad geometry without human demonstrations},
  volume = {625},
  ISSN = {1476-4687},
  //url = {http://dx.doi.org/10.1038/s41586-023-06747-5},
  DOI = {10.1038/s41586-023-06747-5},
  number = {7995},
  journal = {Nature},
  publisher = {Springer Science and Business Media LLC},
  author = {Trinh,  Trieu H. and Wu,  Yuhuai and Le,  Quoc V. and He,  He and Luong,  Thang},
  year = {2024},
  //month = jan,
  pages = {476–482}
}

@article{elliot2008goal,
  title={The goal construct in psychology},
  author={Elliot, Andrew J and Fryer, James W},
  journal={Handbook of motivation science},
  volume={18},
  pages={235--250},
  year={2008}
}



@article{chu2020play,
  title={Play, curiosity, and cognition},
  author={Chu, Junyi and Schulz, Laura E},
  journal={Annual Review of Developmental Psychology},
  volume={2},
  pages={317--343},
  year={2020},
  publisher={Annual Reviews}
}

@misc{chu2023tics,
 title={In Praise of Folly: Flexible Goals and Human Cognition},
 DOI={10.31234/osf.io/zxbqr},
 publisher={PsyArXiv},
 author={Chu, Junyi and Schulz, Laura},
 year={2023},
}

@article{chu2023not,
  title={Not playing by the rules: Exploratory play, rational action, and efficient search},
  author={Chu, Junyi and Schulz, Laura E},
  journal={Open Mind},
  volume={7},
  pages={294--317},
  year={2023}
}

@article{ames1984goal,
  title={Goal structures and motivation},
  author={Ames, Carole and Ames, Russell},
  journal={The Elementary School Journal},
  volume={85},
  number={1},
  pages={39--52},
  year={1984},
  publisher={University of Chicago Press}
}

@article{elliott1988goals,
  title={Goals: An approach to motivation and achievement.},
  author={Elliott, Elaine S and Dweck, Carol S},
  journal={Journal of personality and social psychology},
  volume={54},
  number={1},
  pages={5},
  year={1988},
  publisher={American Psychological Association}
}

@article{fishbach2007goal,
  title={The goal construct in social psychology},
  author={Fishbach, Ayelet and Ferguson, Melissa J},
  journal={Social psychology: Handbook of basic principles},
  volume={2},
  pages={490--515},
  year={2007}
}

@article{locke2002building,
  title={Building a practically useful theory of goal setting and task motivation: A 35-year odyssey.},
  author={Locke, Edwin A and Latham, Gary P},
  journal={American Psychologist},
  volume={57},
  number={9},
  pages={705},
  year={2002},
  publisher={American Psychological Association}
}

@inproceedings{davidson2022creativity,
  title={Creativity, Compositionality, and Common Sense in Human Goal Generation},
  author={Davidson, Guy and Gureckis, Todd M and Lake, Brenden},
  booktitle={Proceedings of the 44th {Annual} {Meeting} of the {Cognitive Science Society}},
editor ={J. Culbertson and A. Perfors and H. Rabagliati and V. Ramenzoni},
  volume={44},
  number={44},
  year={2022}
}

@misc{yang2022re3,
      title={Re3: Generating Longer Stories With Recursive Reprompting and Revision}, 
      author={Kevin Yang and Yuandong Tian and Nanyun Peng and Dan Klein},
      year={2022},
      eprint={2210.06774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Hennessey2010,
  title = {Creativity},
  volume = {61},
  ISSN = {1545-2085},
  DOI = {10.1146/annurev.psych.093008.100416},
  number = {1},
  journal = {Annual Review of Psychology},
  publisher = {Annual Reviews},
  author = {Hennessey,  Beth A. and Amabile,  Teresa M.},
  year = {2010},
  pages = {569–598}
}

@article{ReiterPalmon2009,
  title = {Problem identification and construction: What do we know,  what is the future?},
  volume = {3},
  ISSN = {1931-3896},
  url = {http://dx.doi.org/10.1037/a0014629},
  DOI = {10.1037/a0014629},
  number = {1},
  journal = {Psychology of Aesthetics,  Creativity,  and the Arts},
  publisher = {American Psychological Association (APA)},
  author = {Reiter-Palmon,  Roni and Robinson,  Erika J.},
  year = {2009},
  month = feb,
  pages = {43–47}
}

@article{beaty2012ideas,
  title={Why do ideas get more creative across time? An executive interpretation of the serial order effect in divergent thinking tasks.},
  author={Beaty, Roger E and Silvia, Paul J},
  journal={Psychology of aesthetics, creativity, and the arts},
  volume={6},
  number={4},
  pages={309},
  year={2012},
  publisher={Educational Publishing Foundation}
}

% A* algorithm
@article{hart1968formal,
  title={A formal basis for the heuristic determination of minimum cost paths},
  author={Hart, Peter E and Nilsson, Nils J and Raphael, Bertram},
  journal={IEEE transactions on Systems Science and Cybernetics},
  volume={4},
  number={2},
  pages={100--107},
  year={1968},
  publisher={IEEE}
}

@article{Clark2018,
  title = {Why rate when you could compare? Using the “EloChoice” package to assess pairwise comparisons of perceived physical strength},
  volume = {13},
  ISSN = {1932-6203},
  number = {1},
  journal = {PLOS ONE},
  publisher = {Public Library of Science (PLoS)},
  author = {Clark,  Andrew P. and Howard,  Kate L. and Woods,  Andy T. and Penton-Voak,  Ian S. and Neumann,  Christof},
  editor = {Puebla,  Iratxe},
  year = {2018},
  month = jan,
  pages = {e0190393}
}

  %url = {http://dx.doi.org/10.1371/journal.pone.0190393},
  %DOI = {10.1371/journal.pone.0190393},

# intro nyt digits
@article{Amlen.2023,
 author  = {Amlen, Deb},
 date    = {2023-04-23},
 year = {2023},
 month = apr,
 title   = {How We Make Games at The Times},
 journal = {The New York Times}
}
 %url     = {https://www.nytimes.com/2023/04/10/crosswords/games-digits-beta.html}

# sokoban history & wiki
@misc{sokobanwiki,
  title = {Sokoban Wiki},
 publisher = {MediaWiki},
  year={2009},
   url = {http://www.sokobano.de/wiki},
}

@article{jaruvsek2010human,
  title={Human problem solving: Sokoban case study},
  author={Jaru{\v{s}}ek, Petr and Pel{\'a}nek, Radek},
  journal={Technick{\'a} zpr{\'a}va, Fakulta informatiky, Masarykova univerzita, Brno},
  year={2010}
}

# paper that led to gym-sokoban, where a* alg might come from?
@article{racaniere2017imagination,
  title={Imagination-augmented agents for deep reinforcement learning},
  author={Racani{\`e}re, S{\'e}bastien and Weber, Th{\'e}ophane and Reichert, David and Buesing, Lars and Guez, Arthur and Jimenez Rezende, Danilo and Puigdom{\`e}nech Badia, Adri{\`a} and Vinyals, Oriol and Heess, Nicolas and Li, Yujia and others},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

# using CNNs as opposed to a* and other algorithms
@inproceedings{schaa2024predicting,
  title={Predicting Solvability and Difficulty of Sokoban Puzzles},
  author={Schaa, Hans and Del Solar-Zavala, Jose A and Barriga, Nicolas A},
  booktitle={{2024 43rd International Conference of the Chilean Computer Science Society (SCCC)}},
  pages={1--4},
  year={2024},
  organization={IEEE}
}

# sokobaonline.com,
@misc{sokobanonline,
  publisher = {Uphill Studio LLC}, 
  title = {SokobanOnline.com},
  year = {2011},
   url = {www.sokobanonline.com},
  organization = {Uphill Studio LLC}
}

# gym-sokoban
@misc{SchraderSokoban2018,
  author = {Schrader, Max-Philipp B.},
  title = {gym-sokoban},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/mpSchrader/gym-sokoban}},
  commit = {#CommitId}
}

# ASTAR
@inproceedings{todd2023level,
  title={Level generation through large language models},
  author={Todd, Graham and Earle, Sam and Nasir, Muhammad Umair and Green, Michael Cerny and Togelius, Julian},
  booktitle={Proceedings of the 18th International Conference on the Foundations of Digital Games},
  pages={1--8},
  year={2023}
}

# split half parsons
@article{parsons2021splithalf,
  title={Splithalf: Robust estimates of split half reliability},
  author={Parsons, Sam},
  journal={Journal of Open Source Software},
  volume={6},
  number={60},
  pages={3041},
  year={2021}
}

@article{Schmidhuber_2010,
	Author = {Schmidhuber, J{\"u}rgen},
	Date-Added = {2020-06-26 11:53:08 -0400},
	Date-Modified = {2020-06-26 11:53:08 -0400},
	Issn = {1943-0612},
	Journal = {IEEE Transactions on Autonomous Mental Development},
	Month = {Sep},
	Number = {3},
	Pages = {230--247},
	Publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
	Title = {Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990--2010)},
	Volume = {2},
	Year = {2010}}

@misc{braendle2024fun,
 title={Leveling up fun: Learning progress, expectations, and success influence enjoyment in video games},
 publisher={PsyArXiv},
 author={Brändle, Franziska and Wu, Charley M and Schulz, Eric},
 year={2024},
 month={Mar},
 url={osf.io/preprints/psyarxiv/vg8dz_v3},

}

@article{harackiewicz1993achievement,
  title={Achievement goals and intrinsic motivation.},
  author={Harackiewicz, Judith M and Elliot, Andrew J},
  journal={Journal of personality and social psychology},
  volume={65},
  number={5},
  pages={904},
  year={1993},
  publisher={American Psychological Association}
}

@article{vanOpheusden2023,
  title = {Expertise increases planning depth in human gameplay},
  volume = {618},
  ISSN = {1476-4687},
  number = {7967},
  journal = {Nature},
  publisher = {Springer Science and Business Media LLC},
  author = {van Opheusden,  Bas and Kuperwajs,  Ionatan and Galbiati,  Gianni and Bnaya,  Zahy and Li,  Yunqi and Ma,  Wei Ji},
  year = {2023},
  month = may,
  pages = {1000–1005}
}
%  url = {http://dx.doi.org/10.1038/s41586-023-06124-2},

@article{Baranes2014,
  title = {The effects of task difficulty,  novelty and the size of the search space on intrinsically motivated exploration},
  volume = {8},
  ISSN = {1662-453X},
  journal = {Frontiers in Neuroscience},
  publisher = {Frontiers Media SA},
  author = {Baranes,  Adrien F. and Oudeyer,  Pierre-Yves and Gottlieb,  Jacqueline},
  year = {2014},
  month = oct 
}
%   url = {http://dx.doi.org/10.3389/fnins.2014.00317},
% DOI = {10.3389/fnins.2014.00317},

@article{Oudeyer2007,
  title = {Intrinsic Motivation Systems for Autonomous Mental Development},
  volume = {11},
  ISSN = {1089-778X},
  number = {2},
  journal = {IEEE Transactions on Evolutionary Computation},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  author = {Oudeyer,  Pierre-Yves and Kaplan,  Frdric and Hafner,  Verena V.},
  year = {2007},
  month = apr,
  pages = {265–286}
}
%url = {http://dx.doi.org/10.1109/TEVC.2006.890271},
%DOI = {10.1109/tevc.2006.890271},

@article{brandle2023empowerment,
  title={Empowerment contributes to exploration behaviour in a creative video game},
  author={Br{\"a}ndle, Franziska and Stocks, Lena J and Tenenbaum, Joshua B and Gershman, Samuel J and Schulz, Eric},
  journal={Nature Human Behaviour},
  volume={7},
  number={9},
  pages={1481--1489},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{wilson2005pleasures,
  title={The pleasures of uncertainty: prolonging positive moods in ways people do not anticipate.},
  author={Wilson, Timothy D and Centerbar, David B and Kermer, Deborah A and Gilbert, Daniel T},
  journal={Journal of personality and social psychology},
  volume={88},
  number={1},
  pages={5},
  year={2005},
  publisher={American Psychological Association}
}
@book{czikszentmihalyi1990flow,
  title={Flow: The psychology of optimal experience},
  author={Czikszentmihalyi, Mihaly},
  year={1990},
  publisher={New York: Harper \& Row}
}

@article{Deterding2022,
  title = {Mastering uncertainty: A predictive processing account of enjoying uncertain success in video game play},
  volume = {13},
  ISSN = {1664-1078},
  journal = {Frontiers in Psychology},
  publisher = {Frontiers Media SA},
  author = {Deterding,  Sebastian and Andersen,  Marc Malmdorf and Kiverstein,  Julian and Miller,  Mark},
  year = {2022},
  month = jul 
}

%url = {http://dx.doi.org/10.3389/fpsyg.2022.924953},
%DOI = {10.3389/fpsyg.2022.924953},

@article{Andersen2023,
  title = {Play in predictive minds: A cognitive theory of play.},
  volume = {130},
  ISSN = {0033-295X},
  number = {2},
  journal = {Psychological Review},
  publisher = {American Psychological Association (APA)},
  author = {Andersen,  Marc Malmdorf and Kiverstein,  Julian and Miller,  Mark and Roepstorff,  Andreas},
  year = {2023},
  month = mar,
  pages = {462–479}
}

%url = {http://dx.doi.org/10.1037/rev0000369},
%DOI = {10.1037/rev0000369},