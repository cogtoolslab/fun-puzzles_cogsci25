{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"top\"></a>\n",
    "\n",
    "# Download and tidy data\n",
    "\n",
    "## Contents:\n",
    "* [Import Packages + Set up Paths](#import)\n",
    "\n",
    "\n",
    "* [Connect to Mongo](#mongo)  \n",
    "\n",
    "\n",
    "* [Save Dataframes from Each Experiment](#initialize)  \n",
    "    * [Experiment 1.0](#exp1_data): exit survey, solve (test), compare (pre/posttest)\n",
    "\n",
    "\n",
    "* [Read Dataframes Back Up](#read_from_csv)\n",
    "    * Can start here to read from locally stored data and save time\n",
    "\n",
    "\n",
    "* [Summary Statistics](#summary_stats)\n",
    "    * [Summary of the Data](#summary): Also includes some simple exploratory statistical tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"import\"></a> Import Packages + Settings([^](#top))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "import os, sys\n",
    "import pymongo as pm\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "import pandas as pd\n",
    "import json\n",
    "import socket\n",
    "\n",
    "from operator import itemgetter\n",
    "from collections import Counter\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "## add helpers to python path\n",
    "proj_dir =  os.path.abspath('../..')\n",
    "sys.path.append(os.path.join(proj_dir))\n",
    "from utils import sokoban_solvers as ss\n",
    "from utils import analysis_utils as au"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook parameters\n",
    "sns.set_context('talk')\n",
    "sns.set_style('white')\n",
    "\n",
    "project_name = 'fun-puzzles'\n",
    "experiment_name = 'fun-puzzles-exp1'\n",
    "iterationName = 'production2'\n",
    "load_from_mongo = False # if resync data\n",
    "only_completed = True # only keep complete sessions with exit survey\n",
    "\n",
    "do_replacements = False # update used stim to numGames=100?\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "\n",
    "# directory and file hierarchy\n",
    "studyName = 'study2'\n",
    "# data directory\n",
    "data_dir = os.path.join(proj_dir, 'data', studyName)\n",
    "def make_dir_if_not_exists(dir_name):\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "    return dir_name\n",
    "make_dir_if_not_exists(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"mongo\"></a> Connect To Mongo ([^](#top))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terminal, connect to data server by running\n",
    "\n",
    "`ssh -fNL 27017:127.0.0.1:27017 junyichu@cogtoolslab.org`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set vars\n",
    "auth = pd.read_json('../../../auth.json', typ='series') # this auth.json file contains the password\n",
    "# auth = pd.read_json(os.path.join(proj_dir,'auth.json'), typ='series') # this auth.json file contains the password\n",
    "pswd = auth.password\n",
    "user = auth.user\n",
    "host = 'cogtoolslab.org'\n",
    "\n",
    "# have to fix this to be able to analyze from local\n",
    "conn = pm.MongoClient('mongodb://sketchloop:' + pswd + '@127.0.0.1:27017')\n",
    "db = conn[project_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"initialize\"></a> Save Dataframes from Each Experiment ([^](#top))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehension Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehension Checks\n",
    "if load_from_mongo:    \n",
    "    coll = db[experiment_name]\n",
    "    print(\"Fetching comprehension trials...\")\n",
    "    clear_output(wait=True)\n",
    "    compre_cursor = coll.find({'study_metadata.project': project_name,\n",
    "                'study_metadata.experiment': experiment_name,\n",
    "                'study_metadata.iteration':  iterationName,#{'$in': iterationNames},\n",
    "                'study_phase': 'comprehension'})\n",
    "    compre = pd.DataFrame(list(compre_cursor))\n",
    "\n",
    "    compre_tidy = (pd.DataFrame(compre.session_info.tolist())\n",
    "                   .join(pd.DataFrame(compre.response.tolist(), index=compre.index))\n",
    "                   .join(pd.DataFrame(compre[['rt', 'time_elapsed']], index=compre.index))\n",
    "                   )\n",
    "    compre_tidy['comprehension_attempt'] = compre['trial_index'].apply(lambda x: (x/2) - 3) # 8, 10 -> 1, 2\n",
    "    compre_tidy['q0correct'] = compre['trial_index'].apply(lambda x: (x/2) - 3) # 8, 10 -> 1, 2\n",
    "    # code accuracy\n",
    "    compre_tidy['Q0correct'] = compre_tidy.apply(lambda x: 1 if x['condition'] in x['Q0'] else 0, axis=1)\n",
    "    compre_tidy['Q1correct'] = compre_tidy.apply(lambda x: 1 if 'solve' in x['Q1'] else 0, axis=1)\n",
    "    compre_tidy['correct'] = compre_tidy['Q1correct'] * compre_tidy['Q0correct']\n",
    "    \n",
    "    print(\"Correct on first attempt: {}\".format(compre_tidy['correct'][compre_tidy['comprehension_attempt']==1].sum()))\n",
    "    print(\"Correct on second attempt: {}\".format(compre_tidy['correct'][compre_tidy['comprehension_attempt']==2].sum()))\n",
    "    print(\"Incorrect on second attempt: {}\".format(np.sum(compre_tidy['correct'][compre_tidy['comprehension_attempt']==2] == 0)))\n",
    "\n",
    "    compre_tidy.to_csv(os.path.join(data_dir,iterationName+'_comprehension.csv'),index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get exit survey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only participants who completed the entire study will have an exit survey json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_from_mongo:\n",
    "    # Get exit survey and study timing data\n",
    "    print(\"Fetching exit survey...\")\n",
    "    clear_output(wait=True)\n",
    "    e = coll.find({'study_metadata.project': project_name,\n",
    "                'study_metadata.experiment': experiment_name,\n",
    "                'study_metadata.iteration': iterationName,#{'$in': iterationNames},\n",
    "                'study_phase': 'exit survey',\n",
    "                'study_metadata.dev_mode': False})\n",
    "    E = pd.DataFrame(list(e))\n",
    "\n",
    "    # get list of valid game IDs (i.e, subject number)\n",
    "    completed_gameIDs = [val['gameID'] for key, val in E['session_info'].items() if 'gameID' in val]\n",
    "\n",
    "    print(\"There are {} completed games.\".format(len(completed_gameIDs)))\n",
    "\n",
    "    # save as tidy csv\n",
    "    E_tidy = (pd.DataFrame(E.study_metadata.tolist(), index=E.index)\n",
    "            .drop(['dev_mode', 'study_duration', 'compare_stim_duration'], axis=1)\n",
    "            .join(pd.DataFrame(E.session_info.tolist(), index=E.index))\n",
    "            .join(pd.DataFrame(E.session_timing.tolist(), index=E.index))\n",
    "            .join(pd.DataFrame(E.response.tolist(), index=E.index)))\n",
    "    \n",
    "    E_tidy.to_csv(os.path.join(data_dir,iterationName+'_survey.csv'),index=False) # anonymize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get main puzzle-solving data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make two dataframes: \n",
    "\n",
    "- Trial info: one csv that has one row per trial\n",
    "- Trace data: a folder of csvs, one csv per gameID, one row per action (move / undo / restart / cannot step, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trial info dataframe\n",
    "\n",
    "if load_from_mongo:    \n",
    "# ## fetch main solving records that match desire iterationNames, and turn it into a dataframe\n",
    "    print(\"Fetching test (select-solve-rate) phase...\")\n",
    "    clear_output(wait=True)\n",
    "    s = coll.find({'study_metadata.project': project_name,\n",
    "                'study_metadata.experiment': experiment_name,\n",
    "                'study_metadata.iteration':  iterationName,#{'$in': iterationNames},\n",
    "                'study_phase': 'test'})\n",
    "    S = pd.DataFrame(list(s))\n",
    "\n",
    "    # parse trial number\n",
    "    S = S.assign(trialNum = S['progress_prompt'].str.split(' ').str[1])\n",
    "    S.trialNum = S.trialNum.astype(int)\n",
    "    # S = S.assign(gameID = appS['session_info']['gameID'])\n",
    "    S['gameID'] = S.apply(lambda row: row['session_info'].get('gameID'), axis=1)\n",
    "\n",
    "    # Begin building a tidy dataframe\n",
    "    S_tidy = (pd.DataFrame(S.study_metadata.tolist())\n",
    "              .drop(['dev_mode', 'study_duration', 'compare_stim_duration'], axis=1)\n",
    "              .join(pd.DataFrame(S.session_info.tolist(), index=S.index))\n",
    "              .join(S[['study_phase', 'trialNum', 'trial_type', 'response', 'rt', 'solveDuration','steps', 'inputEvents']])\n",
    "              .join(pd.json_normalize(S['stimuli'])))\n",
    "    S_gameIDs = set([val['gameID'] for key, val in S['session_info'].items() if 'gameID' in val])\n",
    "    print(\"Fetched {} jspsych frames from {} games\".format(len(S_tidy), len(S_gameIDs)))\n",
    "\n",
    "    # make one row per trial, not 3  \n",
    "    S_solve= S_tidy.loc[S_tidy['trial_type']=='sokoban-solve',['gameID', 'trialNum', 'solveDuration', 'steps', 'inputEvents']]\n",
    "    # Extract basic metrics of solve performance\n",
    "    S_solve = S_solve.assign(solved = np.where(S_solve['solveDuration'].notna(), 1, 0))\n",
    "    S_solve = S_solve.assign(attempt_nsteps = S_solve['steps'].apply(lambda x: len(x) if isinstance(x, list) else pd.NA))\n",
    "    S_solve['attempt_nInputEvents'] = S_solve['inputEvents'].apply(lambda x: len(x) if isinstance(x, list) else pd.NA)\n",
    "    S_solve['boxesSolved'] = S_solve['inputEvents'].apply(lambda x: au.maxBoxOnGoal(x) if isinstance(x, list) else pd.NA)\n",
    "\n",
    "    \n",
    "    S_solve = S_solve.drop(['steps', 'inputEvents'], axis=1)\n",
    "\n",
    "    S_select = (S_tidy.loc[S_tidy['trial_type']=='sokoban-select',['gameID', 'trialNum', 'response', 'rt']]\n",
    "                .rename(columns={\"response\": \"select_response\", \"rt\": \"select_rt\"}))\n",
    "\n",
    "    S_rate = (S_tidy.loc[S_tidy['trial_type']=='sokoban-rate',['gameID', 'trialNum', 'response', 'rt']]\n",
    "                .rename(columns={\"response\": \"rate_response\", \"rt\": \"rate_rt\"}))\n",
    "    \n",
    "    S_tidy2 = (S_tidy[S_tidy['trial_type']=='sokoban-rate']\n",
    "               .drop(['trial_type', 'response', 'rt', 'solveDuration', 'steps', 'inputEvents'], axis=1)\n",
    "               .merge(S_select)\n",
    "               .merge(S_rate)\n",
    "               .merge(S_solve))\n",
    "    \n",
    "\n",
    "    ## only keep complete sessions\n",
    "    if (only_completed):\n",
    "        if len(S_gameIDs) > len(completed_gameIDs):\n",
    "            S_tidy2 = S_tidy2[S_tidy2['gameID'].isin(completed_gameIDs)]\n",
    "        print(\"Keeping {} trials from {} completed games\".format(len(S_tidy2), len(completed_gameIDs)))\n",
    "    else:\n",
    "        print(\"Keeping {} trials from {} games\".format(len(S_tidy2), len(S_gameIDs)))\n",
    "    \n",
    "    # export to csv\n",
    "    S_tidy2['level_name'] = S_tidy2['level_name'].astype(int, errors='ignore').astype(str)\n",
    "\n",
    "    S_tidy2.to_csv(os.path.join(data_dir,iterationName+'_testTrials.csv'),index=False) # anonymize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now export trace data as one csv per puzzle, organized by gameID. \n",
    "\n",
    "First check: how many trials of trace data do we have per participant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"N Trials per participant with trace data: {}\".format(S_tidy2.value_counts('gameID').value_counts()))\n",
    "\n",
    "print(\"Who doesn't have 8 trials?\")\n",
    "S_tidy2.value_counts('gameID')[S_tidy2.value_counts('gameID') < 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_boards(trace_df, method, board0):\n",
    "    \"\"\"\n",
    "    Create a DataFrame of box traces based on the specified method.\n",
    "    Args:\n",
    "        trace_df (pd.DataFrame): The input DataFrame containing traces.\n",
    "        method (str): The method to create box traces ('from_df' or 'computed').\n",
    "        boxes (optional): Optional boxes data if method is 'computed'.\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame of box traces.\n",
    "    \"\"\"\n",
    "    if method == \"from_df\":\n",
    "        # unlist the boxes variable, so now one row per box\n",
    "        boxes = trace_df.explode(\"boxes\")[\"boxes\"]\n",
    "        boxes2 = (\n",
    "            pd.json_normalize(boxes)\n",
    "            .set_index(boxes.index)\n",
    "            .reset_index(names=\"eventNum\")\n",
    "        )  # get original action index as a separate column\n",
    "        nboxes = len(boxes[0])\n",
    "        boxes2['boxID'] = np.tile(range(1, nboxes + 1), int(len(boxes2) / nboxes)).astype('str') # index refers to event number [0,0,0,1,1,1]; add box number\n",
    "        # pivot wider, so back to one event per row\n",
    "        boxes3 = boxes2.pivot(index='eventNum', columns = 'boxID', values = ['x', 'y', 'onGoal'] )\n",
    "        boxes3.columns = boxes3.columns.to_flat_index().str.join('_') # rename columns to x_1, x_2, etc.\n",
    "        print(boxes3.head(5))\n",
    "        return boxes3.reset_index()\n",
    "    elif method == \"computed\":\n",
    "        if board0 is None:\n",
    "            raise ValueError(\"Level layout not provided\")\n",
    "        \n",
    "        board_stack = [board0]\n",
    "        boards = []\n",
    "            \n",
    "        for i, row in trace_df.iterrows():\n",
    "            if row['action'] == \"restart\":\n",
    "                board_stack=[board0] # clear boards stack\n",
    "            elif row['event'] in [\"cannot_step\" , \"cannot_push\"]:\n",
    "                None # nothing happens to board stack\n",
    "            elif row['action'] == \"undo\":\n",
    "                if len(board_stack) > 1:\n",
    "                    board_stack.pop() # remove last board until only initial board remains\n",
    "            else: # movement happened\n",
    "                new = board_stack[-1].clone()\n",
    "                new.update_board_from_arrow(row['action'].lower())\n",
    "                board_stack.append(new)\n",
    "                \n",
    "            # now update boxes\n",
    "            currentBoard = board_stack[-1].clone()\n",
    "            boards.append(currentBoard)\n",
    "\n",
    "        return boards\n",
    "    else:  \n",
    "        raise ValueError(\"Invalid method specified\")\n",
    "\n",
    "\n",
    "# # testing\n",
    "# game = '2485-47f57e30-a80b-4dbd-a9e6-6b306a349c3f'\n",
    "# trialnum = 7\n",
    "# row = S[(S['gameID'] == game) & (S['trialNum'] == trialnum) & (S['trial_type'] == 'sokoban-solve')]\n",
    "# traces = pd.json_normalize(row['inputEvents'].explode(ignore_index=True)).reset_index(names=\"eventNum\")\n",
    "# # update time column\n",
    "# start_time = row['startTime'].item()\n",
    "# traces['secondsElapsed'] = .001 * (traces['timestamp'] - start_time)\n",
    "# traces.drop(['timestamp'], axis=1, inplace=True)\n",
    "# # get initial board info\n",
    "# board0 = ss.State().stringInitialize(row['stimuli'].values[0]['layout']) # create board object\n",
    "# value0 = board0.getHeuristic()\n",
    "# # update all actions \n",
    "# boards = generate_boards(traces, \"computed\", board0 = board0)\n",
    "# traces['boxes'] = [b.crates for b in boards]\n",
    "# traces['state_key'] = [b.getKey2() for b in boards]\n",
    "# traces['state_value'] = [b.getHeuristic() for b in boards]\n",
    "# traces['restartCount'] = (traces['action'] == 'restart').cumsum()\n",
    "# # traces['value_diff'] = traces['state_value'].diff()\n",
    "# # traces.loc[traces.index[0], 'value_diff'] = traces.loc[traces.index[0], 'state_value'] - value0\n",
    "# # add first row for initial board\n",
    "# initial_row = pd.DataFrame([{'eventNum': None, 'action': None, 'event': None, 'boxes': board0.crates, 'agent.x': board0.player['x'], 'agent.y': board0.player['y'], 'agent.orientation': 'U', 'secondsElapsed': 0, 'state_key': board0.getKey2(), 'state_value': value0, 'restartCount': 0}])\n",
    "# df = pd.concat([initial_row, traces], ignore_index=True).reset_index(names='stateNum')\n",
    "# df.head()\n",
    "# # traces.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_from_mongo:    \n",
    "    print(\"Parsing puzzle-solving traces...\")\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # For each game, create a folder and save one csv per puzzle\n",
    "    # in the format: gameID-xxx_trial-xxx_puzzle-xxx_inputevents.csv\n",
    "    # where each row of the csv has input, game state\n",
    "    # for game in ['0043-c146f09f-8bb2-4ccc-9577-9363bf3422c1']:\n",
    "    for game in completed_gameIDs:\n",
    "        # make dir for each subject\n",
    "        game_dir = os.path.join(data_dir, \"traces\", game)\n",
    "        make_dir_if_not_exists(game_dir)\n",
    "        \n",
    "        for trialnum in range(1, 9): # 1 to 8\n",
    "            row = S[(S['gameID'] == game) & (S['trialNum'] == trialnum) & (S['trial_type'] == 'sokoban-solve')]\n",
    "            if not row.empty:\n",
    "                clear_output(wait=True)\n",
    "                # build out traces object\n",
    "                start_time = row['startTime'].item()\n",
    "                traces = pd.json_normalize(row['inputEvents'].explode(ignore_index=True)).reset_index(names=\"eventNum\")\n",
    "                traces['gameID'] = game\n",
    "                traces['trialNum'] = trialnum\n",
    "                if 'timestamp' in traces.columns:\n",
    "                    print(\"parsing {} trial {}\".format(game, trialnum)) \n",
    "                    # update time column\n",
    "                    traces['secondsElapsed'] = .001 * (traces['timestamp'] - start_time)\n",
    "                    traces.drop('timestamp', axis=1, inplace=True)\n",
    "                    # get initial board info\n",
    "                    board0 = ss.State().stringInitialize(row['stimuli'].values[0]['layout']) # create board object\n",
    "                    value0 = board0.getHeuristic()\n",
    "                    # update all actions \n",
    "                    boards = generate_boards(traces, \"computed\", board0 = board0)\n",
    "                    traces['boxes'] = [b.crates for b in boards]\n",
    "                    traces['state_key'] = [b.getKey2() for b in boards]\n",
    "                    traces['state_value'] = [b.getHeuristic() for b in boards]\n",
    "                    traces['restartCount'] = (traces['action'] == 'restart').cumsum()\n",
    "                    # traces['value_diff'] = traces['state_value'].diff()\n",
    "                    # traces.loc[traces.index[0], 'value_diff'] = traces.loc[traces.index[0], 'state_value'] - value0\n",
    "                    # add first row for initial board\n",
    "                    initial_row = pd.DataFrame([{'gameID': game, 'trialNum': trialnum, 'eventNum': None, 'action': None, 'event': None, 'boxes': board0.crates, 'agent.x': board0.player['x'], 'agent.y': board0.player['y'], 'agent.orientation': 'U', 'secondsElapsed': 0, 'state_key': board0.getKey2(), 'state_value': value0, 'restartCount': 0}])\n",
    "                    df = pd.concat([initial_row, traces], ignore_index=True).reset_index(names='stateNum')\n",
    "                    # merge and export to csv\n",
    "                    traces = (S_tidy2[['gameID', 'condition','trialNum', \n",
    "                                       'stimuli_set', 'author_name', 'collection_name', 'level_name']]\n",
    "                          .merge(df, how='right'))\n",
    "                else:\n",
    "                    traces = (S_tidy2[['gameID', 'condition','trialNum', \n",
    "                                       'stimuli_set', 'author_name', 'collection_name', 'level_name']]\n",
    "                          .merge(traces, how='right'))\n",
    "                # export to csv\n",
    "                traces.to_csv(os.path.join(game_dir, game+\"_test-\"+ str(trialnum) + \"_attemptTrace.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Practice trial trace data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trial info dataframe\n",
    "if load_from_mongo:    \n",
    "# ## fetch main solving records that match desire iterationNames, and turn it into a dataframe\n",
    "    print(\"Fetching practice levels...\")\n",
    "    clear_output(wait=True)\n",
    "    practice_cursor = coll.find({'study_metadata.project': project_name,\n",
    "                'study_metadata.experiment': experiment_name,\n",
    "                'study_metadata.iteration':  iterationName,#{'$in': iterationNames},\n",
    "                'study_phase': 'practice'})\n",
    "    practice = pd.DataFrame(list(practice_cursor))\n",
    "    practice['trialNum'] = practice['trial_index'] - 3\n",
    "\n",
    "    practice_tidy = (pd.DataFrame(practice.session_info.tolist())\n",
    "                   .join(pd.DataFrame(practice[['trialNum','rt','startTime', 'time_elapsed', 'steps', 'inputEvents']], index=practice.index))\n",
    "                   .join(pd.json_normalize(practice['stimuli']))\n",
    "                   )\n",
    "    \n",
    "    practice_tidy = practice_tidy.assign(attempt_nsteps = practice_tidy['steps'].apply(lambda x: len(x) if isinstance(x, list) else pd.NA))\n",
    "    practice_tidy['attempt_nInputEvents'] = practice_tidy['inputEvents'].apply(lambda x: len(x) if isinstance(x, list) else pd.NA)\n",
    "\n",
    "    (practice_tidy.drop(['inputEvents'], axis=1)\n",
    "     .to_csv(os.path.join(data_dir,iterationName+'_practiceTrials.csv'),index=False))\n",
    "    \n",
    "    print(\"Fetched {} practice trials from {} games\".format(len(practice_tidy), len(practice_tidy.gameID.unique())))\n",
    "\n",
    "# NOW EXPORT ATTEMPT TRACES\n",
    "    print(\"Parsing puzzle-solving traces...\")\n",
    "    # clear_output(wait=True)\n",
    "\n",
    "    # For each game, create a folder and save one csv per puzzle\n",
    "    # in the format: gameID-xxx_trial-xxx_puzzle-xxx_inputevents.csv\n",
    "    # where each row of the csv has input, game state\n",
    "\n",
    "    for game in practice_tidy.gameID.unique():\n",
    "        # make dir for each subject        \n",
    "        game_dir = os.path.join(data_dir, \"traces\", game)\n",
    "        make_dir_if_not_exists(game_dir)\n",
    "\n",
    "        for trialnum in range(1, 4):\n",
    "            row = practice_tidy[(practice_tidy['gameID'] == game) & (practice_tidy['trialNum'] == trialnum)]\n",
    "            row['level_name'] = row['level_id'].astype(int, errors='ignore').astype(str)\n",
    "            row['collection_name'] = row['collection_id']\n",
    "            if not row.empty:\n",
    "                print(\"parsing {} trial {}\".format(game, trialnum))\n",
    "                clear_output(wait=True)\n",
    "                # build out traces object\n",
    "                start_time = row['startTime'].item()\n",
    "                traces = pd.json_normalize(row['inputEvents'].explode(ignore_index=True)).reset_index(names=\"eventNum\")\n",
    "                traces['gameID'] = game\n",
    "                if 'timestamp' in traces.columns:\n",
    "                    # update time column\n",
    "                    traces['secondsElapsed'] = .001 * (traces['timestamp'] - start_time)\n",
    "                    traces.drop('timestamp', axis=1, inplace=True)\n",
    "                    # get initial board info\n",
    "                    board0 = ss.State().stringInitialize(row['layout'].values[0]) # create board object\n",
    "                    value0 = board0.getHeuristic()\n",
    "                    # update all actions \n",
    "                    boards = generate_boards(traces, \"computed\", board0 = board0)\n",
    "                    traces['boxes'] = [b.crates for b in boards]\n",
    "                    traces['state_key'] = [b.getKey2() for b in boards]\n",
    "                    traces['state_value'] = [b.getHeuristic() for b in boards]\n",
    "                    traces['restartCount'] = (traces['action'] == 'restart').cumsum()\n",
    "                    # traces['value_diff'] = traces['state_value'].diff()\n",
    "                    # traces.loc[traces.index[0], 'value_diff'] = traces.loc[traces.index[0], 'state_value'] - value0\n",
    "                    # add first row for initial board\n",
    "                    initial_row = pd.DataFrame([{'gameID': game, 'eventNum': None, 'action': None, 'event': None, 'boxes': board0.crates, 'agent.x': board0.player['x'], 'agent.y': board0.player['y'], 'agent.orientation': 'U', 'secondsElapsed': 0, 'state_key': board0.getKey2(), 'state_value': value0, 'restartCount': 0}])\n",
    "                    df = pd.concat([initial_row, traces], ignore_index=True).reset_index(names='stateNum')\n",
    "                    # merge and export to csv\n",
    "                    traces = (row[['gameID', 'condition','trialNum', 'collection_name', 'level_name']]\n",
    "                          .merge(df, how='right'))\n",
    "                else:\n",
    "                    # empty row\n",
    "                    traces = (row[['gameID', 'condition','trialNum', 'collection_name', 'level_name']]\n",
    "                          .merge(traces, how='right'))\n",
    "                # export to csv\n",
    "                traces.to_csv(os.path.join(game_dir, game+\"_practice-\"+ str(trialnum) + \"_attemptTrace.csv\"), index=False)\n",
    "\n",
    "traces.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get comparison trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_from_mongo:\n",
    "    # # ## Now repeat for pre-test and post-test data\n",
    "    print(\"Fetching comparison phase (pretest and posttest)...\")\n",
    "    clear_output(wait=True)\n",
    "    c = coll.find({'study_metadata.project': project_name,\n",
    "                'study_metadata.experiment': experiment_name,\n",
    "                'study_metadata.iteration':  iterationName,#{'$in': iterationNames},\n",
    "                'study_phase': {'$in': ['pretest', 'posttest']}})\n",
    "    C = pd.DataFrame(list(c))\n",
    "\n",
    "\n",
    "    C2 = (C[['study_phase', 'stimuli', 'trial_type', 'response', 'rt', 'rt_done1', 'rt_done2']]\n",
    "            .join(pd.DataFrame(C.study_metadata.tolist(), index=C.index))\n",
    "            .drop(['dev_mode', 'study_duration', 'compare_stim_duration'], axis=1)\n",
    "            .join(pd.DataFrame(C.session_info.tolist(), index=C.index))\n",
    "            .dropna(subset=['response']))\n",
    "\n",
    "    C2 = C2.assign(trialNum = C['progress_prompt'].str.split(' ').str[1])\n",
    "    C2[\"trialNum\"] = C2[\"trialNum\"].astype(int)\n",
    "    C2[\"response\"] = C2[\"response\"].astype(int)\n",
    "    # print(C2['response'].value_counts())\n",
    "\n",
    "\n",
    "    stims = C2.explode('stimuli')['stimuli']\n",
    "    stims = pd.json_normalize(stims).set_index(stims.index).reset_index(names=\"idx\") # get original action index as a separate column\n",
    "    stims['stimID'] = np.tile(['0','1'], int(len(stims)/2))\n",
    "\n",
    "    # # pivot wider, so back to one event per row\n",
    "    stims2 = stims.pivot(index='idx', columns = 'stimID', values = ['collection_name', 'level_name', 'layout'] )\n",
    "    stims2.columns = stims2.columns.to_flat_index().str.join('_') # rename columns to x_1, x_2, etc.\n",
    "\n",
    "    # # merge and export to csv   \n",
    "    C_tidy = (C2[['gameID', 'condition', 'stim_id', 'study_phase', 'trial_type', 'trialNum', 'response', 'rt', 'rt_done1', 'rt_done2']]\n",
    "                .join(stims2, how='inner')) # default to join by index\n",
    "\n",
    "    ## only keep complete sessions\n",
    "    if (only_completed):\n",
    "        C_gameIDs = set([val['gameID'] for key, val in C['session_info'].items() if 'gameID' in val])\n",
    "        if len(C_gameIDs) > len(completed_gameIDs):\n",
    "            C_tidy = C_tidy[C_tidy['gameID'].isin(completed_gameIDs)]\n",
    "\n",
    "    C_tidy.to_csv(os.path.join(data_dir,iterationName+'_compareTrials.csv'),index=False)\n",
    "\n",
    "# check last output\n",
    "C_tidy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exclusions and replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{} completed exit survey\\n Now we look for missing data.\".format(len(completed_gameIDs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"completed: {}\".format(len(practice_tidy[practice_tidy['trialNum']==3])))\n",
    "print(\"has all trace data: {}\".format(len(practice_tidy.value_counts('gameID')[practice_tidy.value_counts('gameID') == 3])))\n",
    "incomplete_practice = pd.DataFrame(practice_tidy.value_counts('gameID')[practice_tidy.value_counts('gameID') < 3])\n",
    "print(\"incomplete/missing traces: {}\".format(incomplete_practice))\n",
    "\n",
    "# add to exclusions\n",
    "incomplete_practice['reasons'] = incomplete_practice['count'].apply(lambda x: \"Only {} practice trials\".format(x))\n",
    "exclusions = incomplete_practice.filter(['gameID', 'reasons']).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passed comprehension\n",
    "passed_comprehension = compre_tidy.query(\"correct==1\")['gameID'].tolist()\n",
    "print(\"passed: {}\".format(len(passed_comprehension)))\n",
    "\n",
    "print(\"When did participants fail the comprehension test?\")\n",
    "print(compre_tidy.groupby(['comprehension_attempt','correct'])['correct'].agg(['count']))\n",
    "\n",
    "# Get gameIDs of participants who failed comprehension\n",
    "failed_comprehension = compre_tidy.query(\"comprehension_attempt==2 & correct==0\")['gameID'].tolist()\n",
    "print(\"GameIDs that failed comprehension:\", failed_comprehension)\n",
    "\n",
    "failed_comprehension = pd.DataFrame(failed_comprehension, columns=['gameID']).assign(reasons=\"Failed comprehension\")\n",
    "print(\"New Exclusions: \\n\", failed_comprehension)\n",
    "# add to exclusions\n",
    "exclusions = pd.concat([exclusions, \n",
    "                        failed_comprehension], \n",
    "                       ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{} completed all 8 test trials\".format(len(S_tidy2.groupby(['gameID'])['rate_response'].agg(['count']).query('count == 8'))))\n",
    "# missing data\n",
    "missing_test_ratings = S_tidy2.groupby(['gameID'])['rate_response'].agg(['count']).query('count < 8')\n",
    "missing_test_ratings.reset_index(inplace=True)  \n",
    "missing_test_ratings['reasons'] = \"test ratings: only \"+ missing_test_ratings['count'].astype(str) +\" trials\"\n",
    "print(\"Missing test ratings: \\n\", missing_test_ratings)\n",
    "\n",
    "\n",
    "missing_test_steps = S_tidy2.groupby(['gameID'])['attempt_nsteps'].agg(['count']).query('count < 8')\n",
    "missing_test_steps.reset_index(inplace=True)  \n",
    "missing_test_steps['reasons'] = \"test attempts: only \"+ missing_test_steps['count'].astype(str) +\" trials\"\n",
    "print(\"Missing test steps: \\n\", missing_test_steps)\n",
    "\n",
    "# # add to exclusions\n",
    "exclusions = pd.concat([exclusions, \n",
    "                        missing_test_ratings[['gameID', 'reasons']],\n",
    "                        missing_test_steps[['gameID', 'reasons']]],  \n",
    "                       ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{} completed all 16 pre and post test trials\".format(len(C_tidy.groupby(['gameID'])['response'].agg(['count']).query('count == 16'))))\n",
    "# missing data\n",
    "missing_comparisons = C_tidy.groupby(['study_phase', 'gameID'])['response'].agg(['count']).query('count < 8')\n",
    "\n",
    "missing_comparisons.reset_index(inplace=True)  \n",
    "missing_comparisons['reasons'] = \"comparison: only \"+missing_comparisons['count'].astype(str)+\" \"+missing_comparisons['study_phase']+\" trials\"\n",
    "\n",
    "print(\"Missing comparison trials:\", missing_comparisons)\n",
    "\n",
    "# # add to exclusions\n",
    "exclusions = pd.concat([exclusions, \n",
    "                        missing_comparisons[['gameID', 'reasons']]], \n",
    "                       ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export exclusion list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Exporting exclusion reasons\")\n",
    "exclusions.to_csv(os.path.join(data_dir,iterationName+'_exclusions.csv'),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Sessions with complete data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_gameIDs = set(completed_gameIDs) - set(exclusions['gameID'])\n",
    "used_stimIDs = E_tidy['stim_id'][E_tidy['gameID'].isin(final_gameIDs)].reset_index(drop=True)\n",
    "\n",
    "stim_cursor = conn['stimuli'][experiment_name]\n",
    "stim_collection = pd.DataFrame(list(stim_cursor.find()))\n",
    "stim_collection['stim_id'] = stim_collection.apply(lambda row: str(row['_id']), axis=1)\n",
    "stim_collection_used = stim_collection[stim_collection['stim_id'].isin(used_stimIDs)]\n",
    "\n",
    "# Counterbalancing so far\n",
    "print(\"{} valid sessions using {} unique stim sequences, after {} requests to Mongo stimuli db\".format(\n",
    "    len(final_gameIDs), len(stim_collection_used), np.sum(len(stim_collection['games']))\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counterbalancing = E_tidy[E_tidy['gameID'].isin(final_gameIDs)].merge(stim_collection_used[['stim_id', 'stimuli_set_order']])\n",
    "print(\"Counterbalancing table of valid sessions:\")\n",
    "print(counterbalancing.pivot_table(index='stimuli_set_order', columns='condition', values='gameID', aggfunc='count'))\n",
    "\n",
    "print(\"Unique stimuli requested and used\")\n",
    "print(stim_collection_used.pivot_table(index='stimuli_set_order', columns='condition', values='numGames', aggfunc='count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Which stimuli were used repeatedly?\")\n",
    "repeated_stims = counterbalancing.value_counts('stim_id')[counterbalancing.value_counts('stim_id') > 1].reset_index()\n",
    "print(repeated_stims)\n",
    "\n",
    "print(\"Which stimuli were not used?\")\n",
    "stim_collection_unused = stim_collection[~stim_collection['stim_id'].isin(used_stimIDs)].reset_index()\n",
    "print(stim_collection_unused.pivot_table(index='stimuli_set_order', columns='condition', values='numGames', aggfunc='count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_replacements:\n",
    "  print(\"Setting used stimuli to numGames = 100\")\n",
    "  from bson.objectid import ObjectId\n",
    "  for stringid in counterbalancing['stim_id']:\n",
    "  # for stringid in stim_collection_unused['stim_id']:\n",
    "  # for stringid in pd.concat([counterbalancing['stim_id'],repeated_stims['stim_id']],ignore_index=True).unique():\n",
    "    stim_cursor.update_one({\n",
    "      '_id': ObjectId(stringid)\n",
    "    },{\n",
    "      '$set': {\n",
    "        'numGames': 100\n",
    "        # 'numGames': 0\n",
    "      }\n",
    "    }, upsert=False)\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Now in MongoDB Stims:\")\n",
    "print(\"{} stim have numGames =100\".format(len(list(stim_cursor.find({'numGames': {'$eq': 100} })))))\n",
    "print(\"{} stim have numGames < 10\".format(len(list(stim_cursor.find({'numGames': {'$lt': 10} })))))  # Check remaining stim in MongoDB:\") \n",
    "print(\"{} stim have numGames <2\".format(len(list(stim_cursor.find({'numGames': {'$lt': 2} })))))  # Check remaining stim in MongoDB:\") \n",
    "\n",
    "# pd.DataFrame(list(stim_cursor.find({'numGames': {'$lt': 10} }))).pivot_table(index='stimuli_set_order', columns='condition', values='_id', aggfunc='count')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"read_from_csv\"></a> Read Dataframes Back Up ([^](#top))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey = pd.read_csv(os.path.join(data_dir,iterationName+'_survey.csv'))\n",
    "test = pd.read_csv(os.path.join(data_dir,iterationName+'_testTrials.csv'))\n",
    "compare = pd.read_csv(os.path.join(data_dir,iterationName+'_compareTrials.csv'))\n",
    "comprehension = pd.read_csv(os.path.join(data_dir,iterationName+'_compareTrials.csv'))\n",
    "exclusions = pd.read_csv(os.path.join(data_dir,iterationName+'_exclusions.csv'))\n",
    "\n",
    "print(\"** {} participants completed the exit survey.\".format(survey.gameID.nunique()))\n",
    "print(survey['condition'].value_counts())\n",
    "print(survey['participantGender'].value_counts())\n",
    "print(\"M_age = {}, SD_age = {}\".format(\n",
    "    np.round(survey['participantYears'].mean(),2), \n",
    "    np.round(survey['participantYears'].std(),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_included = survey[~survey['gameID'].isin(exclusions['gameID'])]\n",
    "print(\"** {} participants have usable data.\".format(survey_included.gameID.nunique()))\n",
    "print(survey_included['condition'].value_counts())\n",
    "print(survey_included['participantGender'].value_counts())\n",
    "print(\"M_age = {}, SD_age = {}\".format(\n",
    "    np.round(survey_included['participantYears'].mean(),2), \n",
    "    np.round(survey_included['participantYears'].std(),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check for complaints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "  display(survey[['condition','technicalDifficultiesComments']].dropna().sort_values(by=['technicalDifficultiesComments']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"summary\"></a> Summary of the Data ([^](#top))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(survey_included.groupby(['experiment', 'iteration', 'condition']).size().reset_index(name='count'))\n",
    "\n",
    "print(\"N Test Trials: \")\n",
    "print(test[~test['gameID'].isin(exclusions['gameID'])].groupby(['condition', 'gameID'])['solved'].agg(['count']).value_counts('count'))\n",
    "\n",
    "# Which stimuli called more than once?\n",
    "survey_included['stim_id'].value_counts()[survey_included['stim_id'].value_counts() > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean solved should be similar across conditions\n",
    "print('Time to solution by condition')\n",
    "print(test[~test['gameID'].isin(exclusions['gameID'])].groupby(['condition'])['solveDuration'].agg(['count', 'mean', 'std']))\n",
    "# print('Solution duration by puzzle')\n",
    "# print(test.groupby(['collection_name','level_name', 'condition'])['solveDuration'].agg(['count', 'mean', 'std']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Solution rate by puzzle')\n",
    "print(test[~test['gameID'].isin(exclusions['gameID'])]\n",
    "      .groupby(['collection_name','level_name'])['solveDuration']\n",
    "      .agg(['count', 'mean', 'std'])\n",
    "      .sort_values(by = 'count'))\n",
    "# print(solve.loc[solve['trial_type']=='sokoban-solve'].groupby(['stim_collectionName', 'stim_levelId'])['solveDuration'].agg(['count', 'mean', 'std']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
